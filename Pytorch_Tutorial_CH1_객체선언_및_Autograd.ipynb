{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial\n",
    "\n",
    "by Eunjikeam<br>\n",
    "\n",
    "[2019.12.18]\n",
    "딥러닝 프레임워크 중 가장 최근에 나온 파잍토치, 요즘은 딥러닝의 성능 면에서는 파이토치를 가장 많이 사용한다고 한다. 필자도 들어가는 프로젝트에서 파이토치를 사용할 예정이기에 튜토리얼을 정리해보려 한다. \n",
    "\n",
    "---------------\n",
    "\n",
    "### Pytorch 의 장점\n",
    "1. **Python First, 깔끔한 코드**<br>\n",
    "    텐서플로와 달리 'Python First'를 표방한 파이토치는 텐서 연산과 같이 속도에 민감한 부분을 제외하고는 대부분의 모듈이 파이썬으로 짜여 있다. 따라서 코드가 깔끔하다. 게다가 텐서플로는 버전업이 빨라 라이브러리의 버저넙에 다라 이전 버전 코드를 사용하기 어려워졌는데, 파이토치는 상대적으로 적어 혼란이 적다. \n",
    "    <br><br>\n",
    "2. **넘파이와 뛰어난 호환성**<br>\n",
    "    이게 파이토치에서 가장 강력하게 주장하는 것이라 하는데, 싸이노와 케라스의 장점인 넘파이와의 호환성이 파이토치에도 들어왔다 한다. 심지어, 텐서를 다루는 메서드들이 거의 넘파이와 동일한 이름과 인터페이스를 지닌다고 한다.~~나는 넘파이의 연산을 많이 안게 아니라 접근하는데 큰 무리는 없었다~~ 기존에 넘파이를 사용하던 사람들은 큰 장점이 될 수 있다고 한다. \n",
    "    <br><br>\n",
    "3. **Autograd**\n",
    "    값을 앞으로 피드포워드 하며 차례차례 계산한 것을 뿐인데, backword()호출 한번에 역전파 알고리즘을 수행할 수 있다. \n",
    "    <br><br>\n",
    "4. **동적 그래프**\n",
    "    텐서플로의 경우 세션이라는 개념이 있어서, 세션이 시작되면 모델 아키텍쳐 등의 연산 그래프 구조를 수정하기가 어렵다. **하지만 파이토치는 연산과 동시에 동적 그래프가 생성되어 매우 자유롭다.** 이것이 파이토치의 최고 장점 중 하나라고 한다. \n",
    "    <br><br>\n",
    "    \n",
    "-------------\n",
    "\n",
    "아래부터는 간단한 연산과 함께 튜토리얼에 따라 딥러닝을 진행해보려 한다. \n",
    "\n",
    "### reference\n",
    "\n",
    "- [Pytorch로 딥러닝하기](https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/deep_learning_60min_blitz.html)\n",
    "- [book] 김기현의 자연어 처리 딥러닝 캠프(파이토치 편)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텐서(Tensors)\n",
    "\n",
    "Tensor는 numpy의 배열인 ndarray와 같은 개념이다. 파이토치에서 연산을 수행하기 위한 가장 기본적인 객체.GPU를 사용한 연산 가속도도 지원한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.7491e+26, 3.0357e+32, 6.3828e+28],\n",
      "        [1.4603e-19, 1.8617e+25, 1.6212e-19],\n",
      "        [1.8617e+25, 7.9438e+08, 3.2604e-12],\n",
      "        [7.4086e+28, 1.9838e+29, 4.3218e+27],\n",
      "        [4.7423e+30, 6.3828e+28, 1.4603e-19]])\n"
     ]
    }
   ],
   "source": [
    "## 초기화되지 않은 5*3행렬 생성\n",
    "\n",
    "x = torch.Tensor(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5505, 0.2278, 0.3019],\n",
      "        [0.4898, 0.1578, 0.6980],\n",
      "        [0.9438, 0.3489, 0.2335],\n",
      "        [0.0740, 0.1424, 0.2968],\n",
      "        [0.6523, 0.8267, 0.2652]])\n"
     ]
    }
   ],
   "source": [
    "# 무작위 숫자로 초기화된 행렬 생성\n",
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()   # 행렬의 크기 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy의 행렬을 텐서로 변환\n",
    "torch.from_numpy(np.array([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .cuda 매소드 사용하여 Tensor를 GPU상으로 옮기기\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = torch.rand(5,3)\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8514, 1.0365, 1.2901],\n",
       "        [1.1650, 0.9084, 1.6822],\n",
       "        [1.0472, 0.3721, 0.7683],\n",
       "        [0.6500, 0.3288, 1.1674],\n",
       "        [1.0773, 1.4373, 0.9114]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd\n",
    "\n",
    "파이토치는 자동으로 미분 및 역전파를 수행하는 autograd 기능을 가진다. 따라서 대부분의 텐서 간 연산들을 크게 신경 쓸 필요 없이 역전파 알고리즘을 수행하는 명령어를 호출하기만 하면 된다.<br>\n",
    "\n",
    "파이토치는 텐서들 간에 연산을 수행할 때마다 동적으로 연산그래프를 생성하여 연산의 결과물이 어떤 텐서로부터 어떤 연산을 통해서 왓는지 추적한다. 그래서 우리가 최종적으로 나온 스칼라에 역전파 알고리즘을 통해 미분을 수행하도록 했을 때, 각 텐서는 자신의 자식 노드에 해당하는 텐서와 연산을 자동으로 찾아 계속해서 역전파 알고리즘을 수행할 수 있도록 한다. <br>\n",
    "\n",
    "이를 실행 기반 정의(define-by-run) 프레임워크라고 한다.즉 코드를 어떻게 자겅하여 실행하느냐에 따라 역전파가 정의되며, 매 단계마다 달라진다는 뜻이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수(Variable)\n",
    "패키지 중심에는 `autograd.Variable` 클래스가 있다. 이는 tensor를 감싸고 있으며, Tensor 기반으로 정의된 것의 대부분의 연산을 지원한다. 계산이 완료된 후 `.backward()`를 호출하여 모든 변화도(gradient, 미분)를 자동으로 계산할 수 있다. <br>\n",
    "\n",
    "`.data`속성을 사용하여 tensor자체에 접근할 수 있으며, 이 변수와 관련된 변화도(미분값)는 `.grad`에 누적된다.<br>\n",
    "\n",
    "![autograd](https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/_images/Variable.png)\n",
    "<p>\n",
    "\n",
    "|요소 | 의미|\n",
    "|-----|-----|\n",
    "|data | tensor |\n",
    "|grad | gradient(미분) |\n",
    "|grad_fn | 연산된 function| \n",
    "\n",
    "Autograd 구현에서 하나 더 중요한 클래스가 바로 `Function`클래스 이다. \n",
    "`Variable`과 `Function`은 상호 연결되어 있으며, 모든 연산과정을 부호화(encode)하여 순환하지 않은 그래프(acyclic graph)를 생성한다. 각 변수는 .`grad_fn`속성을 갖고 있는데, 이는 `Variable`을 생성한 `Function`을 참조하고 있다.(단 사용자가 만든 Variable은 예와로 이때 `grad_fn` 은 `None`이다)\n",
    "\n",
    "도함수를 계산하기 위해서는 `Variable`의 `.backward()`를 호출하면 된다. `Variable`이 스칼라인 경우에는 `backward`에 인자를 정해줄 필요가 없다. 그러나 여러개의 요소를 가지고 있을 때는 tensor의 모양을 `gradiant`의 인자로 지저할 필요가 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 변수 생성\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 변수에 연산 수행\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fcc9c170f98>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y 에 받아서 다른 연산 수행\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분(gradient)\n",
    "\n",
    "`.backward()`는 역전파(backprop)를 진행해서 미분값을 전달한다. `out.backward()` 는 `out.backward(torch.Tensor([1.0]))` 를 하는 것과 똑같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파 수행, 미분값 전달\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad를 노드 개념으로 봐도 괜찮을 듯 하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
