{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 이해하기\n",
    "\n",
    "필자는 본래 R을 많이 써왔던 터라... Python에 대해 지식이 부족했음. 그리고 인공신경망의 이론적 이해는 어느정도 설립이 되어있으나 그걸 직접 구현해 본 적은 없는 상태. 그 상태에서 TF를 이해하기 정말 쉽지 않았음. 기초 확립과 나중에 참고용으로 사용하기 위해 정리 시작.\n",
    "\n",
    "-------------------\n",
    "\n",
    "### 우선, Tensorflow가 뭐야?\n",
    "머신러닝 알고리즘을 구현하고 실행하는 대표적인 프로그래밍 인터페이스. 작성 시기 현재는 2.0버전이라 keras의 구조로 되어 직관적이어 졌다고 하는데, 1.x일 때는 정말 이해하기가 힘들더라...(~~근데 1.x 도 이해를 하고 있어야 하나...? 이젠 안해도 되려나~~)   \n",
    "요즘은 파이토치라는 것도 나와서 그거에도 흥미를 가지긴 했는데 일단은 이거부터 이해...  \n",
    "텐서플로의 주요 특징 중 하나는 여러개의 GPU를 사용할 수 있는 기능이다. 그래서 대규모 시스템에서 모델을 효율적으로 훈련할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 당시에 사용하던 환경이 아직 1.14.버전이어서 버전을 업데이트함.\n",
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__  ## 2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## 1. 텐서플로의 랭크와 텐서 \n",
    "\n",
    "- **텐서(tensor)** : 데이터를 담고 있는 다차원 배열에 대한 일반화된 수학적 용어\n",
    "- **랭크(rank)** : 텐서의 차원을 나타내는 표기법\n",
    "\n",
    "우리는 보통 데이터를 생각할 때, 스칼라 데이터를 1차원 데이터라고 생각했다. 그러나 텐서의 개념에서는 스칼라는 0차원, 랭크 0인 텐서이다. 그리고 벡터는 랭크 1, 행렬은 랭크 2인 텐서이다. 일반적 분석은 보통 여기서 그칠 때가 많았는데, 텐서는 더 고차원으로 일반화될 수 있다. 특히 이미지분석이나, 자연어처리 분야에서는 3차원, 4차원 텐서를 많이 사용한다.\n",
    "\n",
    "\n",
    "아래는 텐서의 개념을 직관적으로 이해하기 위한 그림이다. \n",
    "\n",
    "\n",
    "![image](https://www.tutorialspoint.com/tensorflow/images/tensor_data_structure.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서를 정의할 때 보통 **`tf.constant()`** 를 사용한다.\n",
    "\n",
    "텐서를 정의하고 랭크를 확인하는 법을 아래에 정리해 보았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크기:  () (4,) (2, 2)\n",
      "랭크:  0 1 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# t1, t2, t3 텐서 정의\n",
    "t1 = tf.constant(np.pi)\n",
    "t2 = tf.constant([1,2,3,4])\n",
    "t3 = tf.constant([[1,2],[3,4]])\n",
    "\n",
    "## rank 구하기\n",
    "r1 = tf.rank(t1)\n",
    "r2 = tf.rank(t2)\n",
    "r3 = tf.rank(t3)\n",
    "\n",
    "## 크기 구하기\n",
    "s1 = t1.get_shape()\n",
    "s2 = t2.get_shape()\n",
    "s3 = t3.get_shape()\n",
    "\n",
    "print('크기: ',s1,s2,s3)\n",
    "print('랭크: ',r1.numpy(),r2.numpy(),r3.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=21, shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 그냥 t2을 치면 어떻게 나올까\n",
    "## 텐서의 정보가 나오면서, shape, array형식의 numpy가 나온다.\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tensor는 그냥 shape()하면 에러가 나는군.적어도 t3.shape (괄호치지 않기)를 해야한다.\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텐서를 다차원 배열로 변환하기\n",
    "\n",
    "종종 텐서의 형태를 변환해야 하는 때가 온다. 그래서 텐서의 형태를 우선 알기 위해 **`tf.get_shape()`**로 텐서의 구조를 알고, **`tf.reshape()`** 를 거쳐 텐서의 구조를 다양하게 변환할 수 있다. \n",
    "\n",
    "- **tf.Variable()** : 변수 생성\n",
    "- **tf.reshape()** : 텐서의 구조 변경\n",
    "\n",
    "tf.constant와 tf.Variable의 차이가 무엇인지 몰라서 왜 다르게 사용하는지 이해가 되지 않았었는데, 명확한 차이가 있었다.   \n",
    "`tf.constant`는 상수를 정의함으로써 한번 정해지면 값이 바뀌지 않는다. 그에 반해 `tf.Variable()`은 변수를 정의함으로 값이 바뀔 수 있다. 그래서 그래프를 실행하기 전에 초기화를 반드시 해줘야 한다.(~~tf 2.0 부터는 필요없는듯 하다~~)  참고로 `tf.placeholder()`도 있는데, 그건 그 구조의 텐서 그릇을 만들어 놓는다고 생각하면 될 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.  2.  3.  3.5]\n",
      " [4.  5.  6.  6.5]\n",
      " [7.  8.  9.  9.5]], shape=(3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "## 행렬 정의\n",
    "arr = np.array([[1, 2, 3, 3.5],[4, 5, 6, 6.5],[7, 8, 9, 9.5]])\n",
    "T1 = tf.constant(arr)\n",
    "\n",
    "print(T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float64, numpy=\n",
      "array([[-0.4986922 ,  0.76515099,  0.58030008,  0.69479197],\n",
      "       [-0.09350516, -0.56045798,  0.46856762,  1.73386214],\n",
      "       [ 0.3666204 ,  0.64763814,  0.47014847, -0.37672242]])>\n",
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float64, numpy=array([0.06380211, 0.54121533, 0.18022827])>\n"
     ]
    }
   ],
   "source": [
    "s = T1.get_shape()                            ## 텐서의 구조 저장\n",
    "\n",
    "T2 = tf.Variable(np.random.normal(size = s))  ## 같은 구조의 랜덤 숫자가 있는 텐서 생성\n",
    "print(T2)\n",
    "\n",
    "T3 = tf.Variable(np.random.normal(size = s[0])) # T1의 첫번째 shape만 가져오는 텐서 생성\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[1.  2.  3.  3.5 4.  5.  6.  6.5 7.  8.  9.  9.5]]], shape=(1, 1, 12), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[[1.  2.  3.  3.5]\n",
      "  [4.  5.  6.  6.5]\n",
      "  [7.  8.  9.  9.5]]], shape=(1, 3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "## 텐서 구조 변경하기\n",
    "T4 = tf.reshape(T1, shape = [1,1,-1]) ## -1은 나머지 차원을 구성된 원소를 바탕으로 자동으로 할당되게 한다.\n",
    "print(T4)\n",
    "\n",
    "T5 = tf.reshape(T1, shape = [1,3,-1])\n",
    "print(T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf에서 행렬을 전치해야 하는 경우도 있는데, 그럴 때는 **`tf.transepose()`**를 사용한다. 일반적인 전치 연산 외에 **perm = [...]**을 이용하여 원하는 순서대로 차원을 지정하여 바꿀 수도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. ]\n",
      "  [4. ]\n",
      "  [7. ]]\n",
      "\n",
      " [[2. ]\n",
      "  [5. ]\n",
      "  [8. ]]\n",
      "\n",
      " [[3. ]\n",
      "  [6. ]\n",
      "  [9. ]]\n",
      "\n",
      " [[3.5]\n",
      "  [6.5]\n",
      "  [9.5]]], shape=(4, 3, 1), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[[1.  4.  7. ]\n",
      "  [2.  5.  8. ]\n",
      "  [3.  6.  9. ]\n",
      "  [3.5 6.5 9.5]]], shape=(1, 4, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T6 = tf.transpose(T5, perm = [2,1,0])  # 2번째,1번째,0번째 순으로 전치\n",
    "print(T6)                              # shape : (1,3,4) ---> (4,3,1)\n",
    "\n",
    "T7 = tf.transpose(T5, perm = [0,2,1])  # 0번째,1번째,2번째 순으로 전치\n",
    "print(T7)                              # shape : (1,3,4) ---> (1,4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서를 split하여 리스트로 나눌 수도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=70, shape=(1, 3, 2), dtype=float64, numpy=\n",
       " array([[[1., 2.],\n",
       "         [4., 5.],\n",
       "         [7., 8.]]])>,\n",
       " <tf.Tensor: id=71, shape=(1, 3, 2), dtype=float64, numpy=\n",
       " array([[[3. , 3.5],\n",
       "         [6. , 6.5],\n",
       "         [9. , 9.5]]])>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_split = tf.split(T5, num_or_size_splits= 2, axis=2)\n",
    "t5_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.concat`**함수를 사용하여 dtype이 같은 텐서 리스트를 연결할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(10, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.ones(shape = (5,1), dtype = tf.float32)\n",
    "t2 = tf.zeros(shape = (5,1), dtype = tf.float32)\n",
    "\n",
    "t3 = tf.concat([t1,t2], axis = 0)  # 0번째 차원 기준 결합\n",
    "t4 = tf.concat([t1,t2], axis = 1)  # 1번째 차원 기준 결합\n",
    "\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 계산 그래프의 이해\n",
    "\n",
    "텐서플로 2.x 버전에서는 즉시 실행 모드가 기본으로 활성화되어 있기 때문에 계산 그래프를 만들지 않고 빠르게 개발과 테스트를 할 수 있다. \n",
    "텐서플로 1.x 버전은 계산 그래프를 만든 후 세션을 통해 그래프를 실행한다.   \n",
    ">1. 비어 있는 새로운 계산 그래프를 만든다.  \n",
    "2. 계산 그래프에 노드(텐서와 연산) 을 추가한다.\n",
    "3. 그래프를 실행한다  \n",
    "    1) 새로운 세션을 시작한다.  \n",
    "    2) 그래프에 있는 변수를 초기화한다.  \n",
    "    3) 이 세션에서 계산 그래프를 실행한다.  \n",
    "    \n",
    "그러나 2.x는 **``tf.function``** 데코레이터를 사용하여 일반 파이선 함수를 호출 가능한 그래프 객체로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(a-b)+c => 1\n"
     ]
    }
   ],
   "source": [
    "## 그래프 생성하기\n",
    "@tf.function             # 함수 호출\n",
    "\n",
    "def simple_func():\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c = tf.constant(3)\n",
    "    \n",
    "    z = 2*(a-b) + c\n",
    "    \n",
    "    return z\n",
    "\n",
    "print('2*(a-b)+c =>',simple_func().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(a-b)+c => 1\n"
     ]
    }
   ],
   "source": [
    "def simple_func():\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c = tf.constant(3)\n",
    "    \n",
    "    z = 2*(a-b) + c\n",
    "    \n",
    "    return z\n",
    "\n",
    "simple_func = tf.function(simple_func)   ## @tf.function 이 아니더라도 이렇게 사용 가능\n",
    "\n",
    "print('2*(a-b)+c =>',simple_func().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Const_1' type=Const>,\n",
       " <tf.Operation 'Const_2' type=Const>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'mul/x' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## simple_func의 자세한 그래프 정의\n",
    "con_func = simple_func.get_concrete_function()\n",
    "con_func.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"Const\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Const_1\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Const_2\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 3\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"sub\"\n",
       "  op: \"Sub\"\n",
       "  input: \"Const\"\n",
       "  input: \"Const_1\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul/x\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul\"\n",
       "  op: \"Mul\"\n",
       "  input: \"mul/x\"\n",
       "  input: \"sub\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"mul\"\n",
       "  input: \"Const_2\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Identity\"\n",
       "  op: \"Identity\"\n",
       "  input: \"add\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 119\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.Variable()`**을 사용하여 수식을 만드는데, 텐서플로 2.x에서는 텐서플로 변수가 파이썬 객체 자체가 되어 이전에 초기화 좌정이 없이도 훨씬 다루기 쉽게 되었다. 그러나 그냥 덧셈을 하는 경우 덧셈이 적용된 상수 텐서를 반환한다. 그냥 변수의 값을 올리고 싶으면, assign()함수를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w2:0' shape=(2, 4) dtype=int64, numpy=\n",
      "array([[1, 2, 3, 4],\n",
      "       [5, 6, 7, 8]])>\n",
      "tf.Tensor(\n",
      "[[2 3 4 5]\n",
      " [6 7 8 9]], shape=(2, 4), dtype=int64)\n",
      "<tf.Variable 'w2:0' shape=(2, 4) dtype=int64, numpy=\n",
      "array([[2, 3, 4, 5],\n",
      "       [6, 7, 8, 9]])>\n"
     ]
    }
   ],
   "source": [
    "## 변수 생성하기\n",
    "w2 = tf.Variable(np.array([[1,2,3,4],\n",
    "                          [5,6,7,8]]), name = 'w2')\n",
    "print(w2)\n",
    "\n",
    "## 변수에 상수를 더하면?\n",
    "print(w2 + 1)\n",
    "\n",
    "## 그냥 변수의 값을 증가시키고 싶다면?\n",
    "w2.assign(w2 + 1)\n",
    "print(w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. tf.keras API \n",
    "\n",
    "### 1) Sequential Model API\n",
    "\n",
    "이제 keras에서 머신러닝 모델을 만드는 고급 방법인 함수형 API를 배워보도록 하자. \n",
    "Sequential 모델을 층을 순서대로 쌓은 모델을 만든다. Dense층을 쌓음 모델을 만들어 합성곱 층과 순환 층을 적용한다. 우선 간단한 회귀분석 모델을 만들어 보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 랜덤한 회귀용 예제 데이터 셋을 만든다.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "def make_random_data():\n",
    "    x = np.random.uniform(low = -2, high = 2, size = 200)\n",
    "    y = []\n",
    "    for t in x : \n",
    "        r = np.random.normal(loc = 0.0, scale = (0.5 + t*t/3), size = None)\n",
    "        y.append(r)\n",
    "        \n",
    "    return x, 1.276*x - 0.84 + np.array(y)\n",
    "\n",
    "x, y = make_random_data()\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train, test set 나누기\n",
    "x_train, y_train = x[:150], y[:150]\n",
    "x_test, y_test = x[150:], y[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 간단 연결층 하나 만들기\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units = 1, input_dim = 1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유닛이 하나 뿐이므로 모델 파라미터 개수는 가중치 1개와 절편 두개 뿐이다. 이제 모델을 컴파일하고 훈련 세트에 학습시켜 보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/500\n",
      "105/105 [==============================] - 1s 6ms/sample - loss: 1.7153 - val_loss: 1.3675\n",
      "Epoch 2/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 1.5685 - val_loss: 1.2772\n",
      "Epoch 3/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 1.4407 - val_loss: 1.1992\n",
      "Epoch 4/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 1.3301 - val_loss: 1.1371\n",
      "Epoch 5/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 1.2393 - val_loss: 1.0838\n",
      "Epoch 6/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 1.1626 - val_loss: 1.0420\n",
      "Epoch 7/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 1.1006 - val_loss: 1.0080\n",
      "Epoch 8/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 1.0488 - val_loss: 0.9782\n",
      "Epoch 9/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 1.0027 - val_loss: 0.9511\n",
      "Epoch 10/500\n",
      "105/105 [==============================] - 0s 252us/sample - loss: 0.9605 - val_loss: 0.9329\n",
      "Epoch 11/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.9331 - val_loss: 0.9178\n",
      "Epoch 12/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.9019 - val_loss: 0.9056\n",
      "Epoch 13/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.8796 - val_loss: 0.8954\n",
      "Epoch 14/500\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.8611 - val_loss: 0.8884\n",
      "Epoch 15/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.8445 - val_loss: 0.8841\n",
      "Epoch 16/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.8331 - val_loss: 0.8792\n",
      "Epoch 17/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.8180 - val_loss: 0.8750\n",
      "Epoch 18/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.8083 - val_loss: 0.8741\n",
      "Epoch 19/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7998 - val_loss: 0.8724\n",
      "Epoch 20/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7956 - val_loss: 0.8700\n",
      "Epoch 21/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7853 - val_loss: 0.8681\n",
      "Epoch 22/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7814 - val_loss: 0.8689\n",
      "Epoch 23/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7807 - val_loss: 0.8683\n",
      "Epoch 24/500\n",
      "105/105 [==============================] - 0s 287us/sample - loss: 0.7798 - val_loss: 0.8707\n",
      "Epoch 25/500\n",
      "105/105 [==============================] - 0s 289us/sample - loss: 0.7730 - val_loss: 0.8718\n",
      "Epoch 26/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7758 - val_loss: 0.8739\n",
      "Epoch 27/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7683 - val_loss: 0.8766\n",
      "Epoch 28/500\n",
      "105/105 [==============================] - 0s 252us/sample - loss: 0.7671 - val_loss: 0.8782\n",
      "Epoch 29/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7663 - val_loss: 0.8808\n",
      "Epoch 30/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7643 - val_loss: 0.8797\n",
      "Epoch 31/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7630 - val_loss: 0.8825\n",
      "Epoch 32/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7639 - val_loss: 0.8799\n",
      "Epoch 33/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7607 - val_loss: 0.8794\n",
      "Epoch 34/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7631 - val_loss: 0.8776\n",
      "Epoch 35/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7616 - val_loss: 0.8797\n",
      "Epoch 36/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7601 - val_loss: 0.8820\n",
      "Epoch 37/500\n",
      "105/105 [==============================] - 0s 317us/sample - loss: 0.7602 - val_loss: 0.8809\n",
      "Epoch 38/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7610 - val_loss: 0.8826\n",
      "Epoch 39/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7596 - val_loss: 0.8864\n",
      "Epoch 40/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7593 - val_loss: 0.8849\n",
      "Epoch 41/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7593 - val_loss: 0.8856\n",
      "Epoch 42/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7594 - val_loss: 0.8882\n",
      "Epoch 43/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7580 - val_loss: 0.8886\n",
      "Epoch 44/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7584 - val_loss: 0.8929\n",
      "Epoch 45/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7598 - val_loss: 0.8961\n",
      "Epoch 46/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7573 - val_loss: 0.9005\n",
      "Epoch 47/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7591 - val_loss: 0.9017\n",
      "Epoch 48/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7576 - val_loss: 0.8987\n",
      "Epoch 49/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7560 - val_loss: 0.8989\n",
      "Epoch 50/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.7567 - val_loss: 0.8974\n",
      "Epoch 51/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7561 - val_loss: 0.8965\n",
      "Epoch 52/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7561 - val_loss: 0.8945\n",
      "Epoch 53/500\n",
      "105/105 [==============================] - 0s 170us/sample - loss: 0.7572 - val_loss: 0.8942\n",
      "Epoch 54/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7565 - val_loss: 0.8945\n",
      "Epoch 55/500\n",
      "105/105 [==============================] - 0s 276us/sample - loss: 0.7563 - val_loss: 0.8934\n",
      "Epoch 56/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7567 - val_loss: 0.8943\n",
      "Epoch 57/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7569 - val_loss: 0.8927\n",
      "Epoch 58/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7563 - val_loss: 0.8918\n",
      "Epoch 59/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7570 - val_loss: 0.8934\n",
      "Epoch 60/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7563 - val_loss: 0.8956\n",
      "Epoch 61/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7564 - val_loss: 0.8966\n",
      "Epoch 62/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 0.7583 - val_loss: 0.8944\n",
      "Epoch 63/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7568 - val_loss: 0.8974\n",
      "Epoch 64/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7564 - val_loss: 0.8957\n",
      "Epoch 65/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7570 - val_loss: 0.8952\n",
      "Epoch 66/500\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 0.7562 - val_loss: 0.8938\n",
      "Epoch 67/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7568 - val_loss: 0.8952\n",
      "Epoch 68/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7567 - val_loss: 0.8996\n",
      "Epoch 69/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7564 - val_loss: 0.9018\n",
      "Epoch 70/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7562 - val_loss: 0.9033\n",
      "Epoch 71/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7567 - val_loss: 0.9041\n",
      "Epoch 72/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7561 - val_loss: 0.9052\n",
      "Epoch 73/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7564 - val_loss: 0.9128\n",
      "Epoch 74/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7568 - val_loss: 0.9116\n",
      "Epoch 75/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7560 - val_loss: 0.9088\n",
      "Epoch 76/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7587 - val_loss: 0.9127\n",
      "Epoch 77/500\n",
      "105/105 [==============================] - 0s 272us/sample - loss: 0.7563 - val_loss: 0.9118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7559 - val_loss: 0.9100\n",
      "Epoch 79/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7565 - val_loss: 0.9133\n",
      "Epoch 80/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7562 - val_loss: 0.9126\n",
      "Epoch 81/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7579 - val_loss: 0.9109\n",
      "Epoch 82/500\n",
      "105/105 [==============================] - 0s 312us/sample - loss: 0.7568 - val_loss: 0.9143\n",
      "Epoch 83/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7580 - val_loss: 0.9140\n",
      "Epoch 84/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7575 - val_loss: 0.9139\n",
      "Epoch 85/500\n",
      "105/105 [==============================] - 0s 282us/sample - loss: 0.7564 - val_loss: 0.9123\n",
      "Epoch 86/500\n",
      "105/105 [==============================] - 0s 174us/sample - loss: 0.7569 - val_loss: 0.9111\n",
      "Epoch 87/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7566 - val_loss: 0.9096\n",
      "Epoch 88/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7575 - val_loss: 0.9131\n",
      "Epoch 89/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7568 - val_loss: 0.9181\n",
      "Epoch 90/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7569 - val_loss: 0.9150\n",
      "Epoch 91/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7572 - val_loss: 0.9150\n",
      "Epoch 92/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7566 - val_loss: 0.9135\n",
      "Epoch 93/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7561 - val_loss: 0.9112\n",
      "Epoch 94/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7564 - val_loss: 0.9078\n",
      "Epoch 95/500\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 0.7562 - val_loss: 0.9095\n",
      "Epoch 96/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7563 - val_loss: 0.9093\n",
      "Epoch 97/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7566 - val_loss: 0.9070\n",
      "Epoch 98/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7577 - val_loss: 0.9011\n",
      "Epoch 99/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7588 - val_loss: 0.9030\n",
      "Epoch 100/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7582 - val_loss: 0.9052\n",
      "Epoch 101/500\n",
      "105/105 [==============================] - 0s 302us/sample - loss: 0.7567 - val_loss: 0.9041\n",
      "Epoch 102/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7560 - val_loss: 0.9069\n",
      "Epoch 103/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7577 - val_loss: 0.9033\n",
      "Epoch 104/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7565 - val_loss: 0.9095\n",
      "Epoch 105/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7561 - val_loss: 0.9122\n",
      "Epoch 106/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7576 - val_loss: 0.9163\n",
      "Epoch 107/500\n",
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7572 - val_loss: 0.9204\n",
      "Epoch 108/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7583 - val_loss: 0.9206\n",
      "Epoch 109/500\n",
      "105/105 [==============================] - 0s 276us/sample - loss: 0.7584 - val_loss: 0.9197\n",
      "Epoch 110/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7568 - val_loss: 0.9194\n",
      "Epoch 111/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7581 - val_loss: 0.9175\n",
      "Epoch 112/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7568 - val_loss: 0.9244\n",
      "Epoch 113/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7577 - val_loss: 0.9206\n",
      "Epoch 114/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7572 - val_loss: 0.9161\n",
      "Epoch 115/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7562 - val_loss: 0.9152\n",
      "Epoch 116/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7563 - val_loss: 0.9120\n",
      "Epoch 117/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7561 - val_loss: 0.9114\n",
      "Epoch 118/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7569 - val_loss: 0.9115\n",
      "Epoch 119/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7561 - val_loss: 0.9112\n",
      "Epoch 120/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7565 - val_loss: 0.9115\n",
      "Epoch 121/500\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 0.7575 - val_loss: 0.9120\n",
      "Epoch 122/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7569 - val_loss: 0.9093\n",
      "Epoch 123/500\n",
      "105/105 [==============================] - 0s 566us/sample - loss: 0.7578 - val_loss: 0.9084\n",
      "Epoch 124/500\n",
      "105/105 [==============================] - 0s 822us/sample - loss: 0.7562 - val_loss: 0.9076\n",
      "Epoch 125/500\n",
      "105/105 [==============================] - 0s 2ms/sample - loss: 0.7565 - val_loss: 0.9140\n",
      "Epoch 126/500\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.7581 - val_loss: 0.9132\n",
      "Epoch 127/500\n",
      "105/105 [==============================] - 0s 301us/sample - loss: 0.7566 - val_loss: 0.9083\n",
      "Epoch 128/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7588 - val_loss: 0.9098\n",
      "Epoch 129/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7565 - val_loss: 0.9049\n",
      "Epoch 130/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7572 - val_loss: 0.9058\n",
      "Epoch 131/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7560 - val_loss: 0.9061\n",
      "Epoch 132/500\n",
      "105/105 [==============================] - 0s 170us/sample - loss: 0.7563 - val_loss: 0.9038\n",
      "Epoch 133/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7564 - val_loss: 0.9056\n",
      "Epoch 134/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7565 - val_loss: 0.9086\n",
      "Epoch 135/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7562 - val_loss: 0.9055\n",
      "Epoch 136/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7562 - val_loss: 0.9036\n",
      "Epoch 137/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7561 - val_loss: 0.9045\n",
      "Epoch 138/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7567 - val_loss: 0.9053\n",
      "Epoch 139/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7566 - val_loss: 0.9052\n",
      "Epoch 140/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7562 - val_loss: 0.9052\n",
      "Epoch 141/500\n",
      "105/105 [==============================] - 0s 176us/sample - loss: 0.7559 - val_loss: 0.9045\n",
      "Epoch 142/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7566 - val_loss: 0.9044\n",
      "Epoch 143/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.7562 - val_loss: 0.9037\n",
      "Epoch 144/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7572 - val_loss: 0.9138\n",
      "Epoch 145/500\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 0.7574 - val_loss: 0.9111\n",
      "Epoch 146/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7572 - val_loss: 0.9102\n",
      "Epoch 147/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7583 - val_loss: 0.9065\n",
      "Epoch 148/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7567 - val_loss: 0.9091\n",
      "Epoch 149/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7560 - val_loss: 0.9086\n",
      "Epoch 150/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.9084\n",
      "Epoch 151/500\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.7561 - val_loss: 0.9024\n",
      "Epoch 152/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7567 - val_loss: 0.9077\n",
      "Epoch 153/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7571 - val_loss: 0.9123\n",
      "Epoch 154/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7571 - val_loss: 0.9117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7569 - val_loss: 0.9170\n",
      "Epoch 156/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7575 - val_loss: 0.9162\n",
      "Epoch 157/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7569 - val_loss: 0.9186\n",
      "Epoch 158/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7590 - val_loss: 0.9196\n",
      "Epoch 159/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7585 - val_loss: 0.9158\n",
      "Epoch 160/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7574 - val_loss: 0.9115\n",
      "Epoch 161/500\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.7571 - val_loss: 0.9117\n",
      "Epoch 162/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7563 - val_loss: 0.9140\n",
      "Epoch 163/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7569 - val_loss: 0.9141\n",
      "Epoch 164/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7567 - val_loss: 0.9082\n",
      "Epoch 165/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7565 - val_loss: 0.9083\n",
      "Epoch 166/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7558 - val_loss: 0.9111\n",
      "Epoch 167/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7580 - val_loss: 0.9077\n",
      "Epoch 168/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7559 - val_loss: 0.9043\n",
      "Epoch 169/500\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 0.7557 - val_loss: 0.9058\n",
      "Epoch 170/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7564 - val_loss: 0.9043\n",
      "Epoch 171/500\n",
      "105/105 [==============================] - 0s 314us/sample - loss: 0.7557 - val_loss: 0.9036\n",
      "Epoch 172/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7562 - val_loss: 0.9084\n",
      "Epoch 173/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7579 - val_loss: 0.9130\n",
      "Epoch 174/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7561 - val_loss: 0.9095\n",
      "Epoch 175/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7557 - val_loss: 0.9067\n",
      "Epoch 176/500\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.7573 - val_loss: 0.9050\n",
      "Epoch 177/500\n",
      "105/105 [==============================] - 0s 276us/sample - loss: 0.7561 - val_loss: 0.9048\n",
      "Epoch 178/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7559 - val_loss: 0.9044\n",
      "Epoch 179/500\n",
      "105/105 [==============================] - 0s 281us/sample - loss: 0.7563 - val_loss: 0.9062\n",
      "Epoch 180/500\n",
      "105/105 [==============================] - 0s 353us/sample - loss: 0.7571 - val_loss: 0.9028\n",
      "Epoch 181/500\n",
      "105/105 [==============================] - 0s 315us/sample - loss: 0.7577 - val_loss: 0.9057\n",
      "Epoch 182/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7564 - val_loss: 0.9089\n",
      "Epoch 183/500\n",
      "105/105 [==============================] - 0s 333us/sample - loss: 0.7576 - val_loss: 0.9088\n",
      "Epoch 184/500\n",
      "105/105 [==============================] - 0s 263us/sample - loss: 0.7559 - val_loss: 0.9098\n",
      "Epoch 185/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.7564 - val_loss: 0.9109\n",
      "Epoch 186/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7561 - val_loss: 0.9088\n",
      "Epoch 187/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7566 - val_loss: 0.9081\n",
      "Epoch 188/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7579 - val_loss: 0.9117\n",
      "Epoch 189/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7575 - val_loss: 0.9168\n",
      "Epoch 190/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7563 - val_loss: 0.9134\n",
      "Epoch 191/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7565 - val_loss: 0.9096\n",
      "Epoch 192/500\n",
      "105/105 [==============================] - 0s 421us/sample - loss: 0.7563 - val_loss: 0.9126\n",
      "Epoch 193/500\n",
      "105/105 [==============================] - 0s 288us/sample - loss: 0.7571 - val_loss: 0.9170\n",
      "Epoch 194/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7567 - val_loss: 0.9183\n",
      "Epoch 195/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7571 - val_loss: 0.9215\n",
      "Epoch 196/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7568 - val_loss: 0.9165\n",
      "Epoch 197/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7575 - val_loss: 0.9152\n",
      "Epoch 198/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7564 - val_loss: 0.9152\n",
      "Epoch 199/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7572 - val_loss: 0.9170\n",
      "Epoch 200/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7572 - val_loss: 0.9148\n",
      "Epoch 201/500\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 0.7560 - val_loss: 0.9134\n",
      "Epoch 202/500\n",
      "105/105 [==============================] - 0s 291us/sample - loss: 0.7567 - val_loss: 0.9172\n",
      "Epoch 203/500\n",
      "105/105 [==============================] - 0s 273us/sample - loss: 0.7566 - val_loss: 0.9156\n",
      "Epoch 204/500\n",
      "105/105 [==============================] - 0s 289us/sample - loss: 0.7578 - val_loss: 0.9205\n",
      "Epoch 205/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7582 - val_loss: 0.9154\n",
      "Epoch 206/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7568 - val_loss: 0.9120\n",
      "Epoch 207/500\n",
      "105/105 [==============================] - 0s 282us/sample - loss: 0.7573 - val_loss: 0.9063\n",
      "Epoch 208/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7576 - val_loss: 0.9035\n",
      "Epoch 209/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7574 - val_loss: 0.9019\n",
      "Epoch 210/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7562 - val_loss: 0.9063\n",
      "Epoch 211/500\n",
      "105/105 [==============================] - 0s 273us/sample - loss: 0.7559 - val_loss: 0.9077\n",
      "Epoch 212/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7560 - val_loss: 0.9046\n",
      "Epoch 213/500\n",
      "105/105 [==============================] - 0s 268us/sample - loss: 0.7557 - val_loss: 0.9044\n",
      "Epoch 214/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7559 - val_loss: 0.9058\n",
      "Epoch 215/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7560 - val_loss: 0.9035\n",
      "Epoch 216/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7568 - val_loss: 0.9025\n",
      "Epoch 217/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7567 - val_loss: 0.9041\n",
      "Epoch 218/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7557 - val_loss: 0.9079\n",
      "Epoch 219/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7563 - val_loss: 0.9083\n",
      "Epoch 220/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7563 - val_loss: 0.9089\n",
      "Epoch 221/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7568 - val_loss: 0.9069\n",
      "Epoch 222/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7560 - val_loss: 0.9034\n",
      "Epoch 223/500\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7572 - val_loss: 0.9053\n",
      "Epoch 224/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7567 - val_loss: 0.9028\n",
      "Epoch 225/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7564 - val_loss: 0.9043\n",
      "Epoch 226/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7564 - val_loss: 0.9015\n",
      "Epoch 227/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7567 - val_loss: 0.9026\n",
      "Epoch 228/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7558 - val_loss: 0.9042\n",
      "Epoch 229/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7558 - val_loss: 0.9035\n",
      "Epoch 230/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7570 - val_loss: 0.9026\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7564 - val_loss: 0.9006\n",
      "Epoch 232/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7559 - val_loss: 0.8994\n",
      "Epoch 233/500\n",
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7566 - val_loss: 0.9005\n",
      "Epoch 234/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7559 - val_loss: 0.8993\n",
      "Epoch 235/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7563 - val_loss: 0.8965\n",
      "Epoch 236/500\n",
      "105/105 [==============================] - 0s 311us/sample - loss: 0.7572 - val_loss: 0.8974\n",
      "Epoch 237/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7568 - val_loss: 0.8946\n",
      "Epoch 238/500\n",
      "105/105 [==============================] - 0s 269us/sample - loss: 0.7588 - val_loss: 0.9005\n",
      "Epoch 239/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7575 - val_loss: 0.8999\n",
      "Epoch 240/500\n",
      "105/105 [==============================] - 0s 186us/sample - loss: 0.7564 - val_loss: 0.9054\n",
      "Epoch 241/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7573 - val_loss: 0.9065\n",
      "Epoch 242/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7574 - val_loss: 0.9087\n",
      "Epoch 243/500\n",
      "105/105 [==============================] - 0s 271us/sample - loss: 0.7563 - val_loss: 0.9111\n",
      "Epoch 244/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9140\n",
      "Epoch 245/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7565 - val_loss: 0.9117\n",
      "Epoch 246/500\n",
      "105/105 [==============================] - 0s 166us/sample - loss: 0.7558 - val_loss: 0.9105\n",
      "Epoch 247/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7564 - val_loss: 0.9078\n",
      "Epoch 248/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7571 - val_loss: 0.9111\n",
      "Epoch 249/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7569 - val_loss: 0.9051\n",
      "Epoch 250/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7557 - val_loss: 0.9051\n",
      "Epoch 251/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7561 - val_loss: 0.9030\n",
      "Epoch 252/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7563 - val_loss: 0.9078\n",
      "Epoch 253/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7560 - val_loss: 0.9051\n",
      "Epoch 254/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7565 - val_loss: 0.9053\n",
      "Epoch 255/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7563 - val_loss: 0.9098\n",
      "Epoch 256/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7565 - val_loss: 0.9066\n",
      "Epoch 257/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7560 - val_loss: 0.9109\n",
      "Epoch 258/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7569 - val_loss: 0.9108\n",
      "Epoch 259/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7559 - val_loss: 0.9079\n",
      "Epoch 260/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7576 - val_loss: 0.9080\n",
      "Epoch 261/500\n",
      "105/105 [==============================] - 0s 252us/sample - loss: 0.7557 - val_loss: 0.9091\n",
      "Epoch 262/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7585 - val_loss: 0.9075\n",
      "Epoch 263/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7565 - val_loss: 0.9105\n",
      "Epoch 264/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7556 - val_loss: 0.9105\n",
      "Epoch 265/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7571 - val_loss: 0.9127\n",
      "Epoch 266/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7560 - val_loss: 0.9126\n",
      "Epoch 267/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7569 - val_loss: 0.9145\n",
      "Epoch 268/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7569 - val_loss: 0.9149\n",
      "Epoch 269/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7562 - val_loss: 0.9116\n",
      "Epoch 270/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.9096\n",
      "Epoch 271/500\n",
      "105/105 [==============================] - 0s 287us/sample - loss: 0.7561 - val_loss: 0.9060\n",
      "Epoch 272/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7573 - val_loss: 0.9059\n",
      "Epoch 273/500\n",
      "105/105 [==============================] - 0s 449us/sample - loss: 0.7558 - val_loss: 0.9040\n",
      "Epoch 274/500\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.7572 - val_loss: 0.9058\n",
      "Epoch 275/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7560 - val_loss: 0.9035\n",
      "Epoch 276/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7561 - val_loss: 0.9026\n",
      "Epoch 277/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7568 - val_loss: 0.8993\n",
      "Epoch 278/500\n",
      "105/105 [==============================] - 0s 146us/sample - loss: 0.7560 - val_loss: 0.8990\n",
      "Epoch 279/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7571 - val_loss: 0.8997\n",
      "Epoch 280/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7573 - val_loss: 0.9006\n",
      "Epoch 281/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7562 - val_loss: 0.9032\n",
      "Epoch 282/500\n",
      "105/105 [==============================] - 0s 335us/sample - loss: 0.7572 - val_loss: 0.9006\n",
      "Epoch 283/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7563 - val_loss: 0.9015\n",
      "Epoch 284/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7565 - val_loss: 0.9003\n",
      "Epoch 285/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7558 - val_loss: 0.8994\n",
      "Epoch 286/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7568 - val_loss: 0.8975\n",
      "Epoch 287/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7567 - val_loss: 0.9014\n",
      "Epoch 288/500\n",
      "105/105 [==============================] - 0s 284us/sample - loss: 0.7573 - val_loss: 0.8995\n",
      "Epoch 289/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7567 - val_loss: 0.8989\n",
      "Epoch 290/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7564 - val_loss: 0.8986\n",
      "Epoch 291/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7566 - val_loss: 0.8989\n",
      "Epoch 292/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7559 - val_loss: 0.9005\n",
      "Epoch 293/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7563 - val_loss: 0.8995\n",
      "Epoch 294/500\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.7574 - val_loss: 0.9037\n",
      "Epoch 295/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7558 - val_loss: 0.9021\n",
      "Epoch 296/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7558 - val_loss: 0.9039\n",
      "Epoch 297/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7562 - val_loss: 0.9051\n",
      "Epoch 298/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7574 - val_loss: 0.9030\n",
      "Epoch 299/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7570 - val_loss: 0.9028\n",
      "Epoch 300/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7564 - val_loss: 0.9043\n",
      "Epoch 301/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7571 - val_loss: 0.9032\n",
      "Epoch 302/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7557 - val_loss: 0.9025\n",
      "Epoch 303/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7563 - val_loss: 0.9011\n",
      "Epoch 304/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7564 - val_loss: 0.9027\n",
      "Epoch 305/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7588 - val_loss: 0.9067\n",
      "Epoch 306/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7563 - val_loss: 0.9068\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7568 - val_loss: 0.9086\n",
      "Epoch 308/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7571 - val_loss: 0.9071\n",
      "Epoch 309/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7574 - val_loss: 0.9067\n",
      "Epoch 310/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7568 - val_loss: 0.9113\n",
      "Epoch 311/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7561 - val_loss: 0.9112\n",
      "Epoch 312/500\n",
      "105/105 [==============================] - 0s 189us/sample - loss: 0.7570 - val_loss: 0.9151\n",
      "Epoch 313/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7568 - val_loss: 0.9159\n",
      "Epoch 314/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7566 - val_loss: 0.9145\n",
      "Epoch 315/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.7569 - val_loss: 0.9118\n",
      "Epoch 316/500\n",
      "105/105 [==============================] - 0s 282us/sample - loss: 0.7570 - val_loss: 0.9086\n",
      "Epoch 317/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7566 - val_loss: 0.9120\n",
      "Epoch 318/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7577 - val_loss: 0.9159\n",
      "Epoch 319/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7580 - val_loss: 0.9144\n",
      "Epoch 320/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7572 - val_loss: 0.9157\n",
      "Epoch 321/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7570 - val_loss: 0.9201\n",
      "Epoch 322/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7576 - val_loss: 0.9213\n",
      "Epoch 323/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7577 - val_loss: 0.9187\n",
      "Epoch 324/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7569 - val_loss: 0.9172\n",
      "Epoch 325/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7570 - val_loss: 0.9151\n",
      "Epoch 326/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7580 - val_loss: 0.9160\n",
      "Epoch 327/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7565 - val_loss: 0.9118\n",
      "Epoch 328/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7565 - val_loss: 0.9084\n",
      "Epoch 329/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7565 - val_loss: 0.9076\n",
      "Epoch 330/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7581 - val_loss: 0.9015\n",
      "Epoch 331/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7567 - val_loss: 0.9021\n",
      "Epoch 332/500\n",
      "105/105 [==============================] - 0s 292us/sample - loss: 0.7572 - val_loss: 0.9024\n",
      "Epoch 333/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7582 - val_loss: 0.9015\n",
      "Epoch 334/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7574 - val_loss: 0.8986\n",
      "Epoch 335/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7568 - val_loss: 0.9006\n",
      "Epoch 336/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7579 - val_loss: 0.9012\n",
      "Epoch 337/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7566 - val_loss: 0.9012\n",
      "Epoch 338/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7563 - val_loss: 0.8988\n",
      "Epoch 339/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7573 - val_loss: 0.8986\n",
      "Epoch 340/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7560 - val_loss: 0.8999\n",
      "Epoch 341/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7564 - val_loss: 0.8997\n",
      "Epoch 342/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7560 - val_loss: 0.9018\n",
      "Epoch 343/500\n",
      "105/105 [==============================] - 0s 303us/sample - loss: 0.7572 - val_loss: 0.9066\n",
      "Epoch 344/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7570 - val_loss: 0.9030\n",
      "Epoch 345/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7562 - val_loss: 0.9065\n",
      "Epoch 346/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7562 - val_loss: 0.9128\n",
      "Epoch 347/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7574 - val_loss: 0.9145\n",
      "Epoch 348/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7564 - val_loss: 0.9183\n",
      "Epoch 349/500\n",
      "105/105 [==============================] - 0s 299us/sample - loss: 0.7563 - val_loss: 0.9153\n",
      "Epoch 350/500\n",
      "105/105 [==============================] - 0s 269us/sample - loss: 0.7568 - val_loss: 0.9139\n",
      "Epoch 351/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7572 - val_loss: 0.9132\n",
      "Epoch 352/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7566 - val_loss: 0.9100\n",
      "Epoch 353/500\n",
      "105/105 [==============================] - 0s 321us/sample - loss: 0.7560 - val_loss: 0.9113\n",
      "Epoch 354/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7561 - val_loss: 0.9100\n",
      "Epoch 355/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7568 - val_loss: 0.9076\n",
      "Epoch 356/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7567 - val_loss: 0.9061\n",
      "Epoch 357/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7566 - val_loss: 0.9017\n",
      "Epoch 358/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7571 - val_loss: 0.9058\n",
      "Epoch 359/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7556 - val_loss: 0.9075\n",
      "Epoch 360/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7560 - val_loss: 0.9076\n",
      "Epoch 361/500\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 0.7569 - val_loss: 0.9138\n",
      "Epoch 362/500\n",
      "105/105 [==============================] - 0s 387us/sample - loss: 0.7564 - val_loss: 0.9144\n",
      "Epoch 363/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7568 - val_loss: 0.9098\n",
      "Epoch 364/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7567 - val_loss: 0.9076\n",
      "Epoch 365/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7561 - val_loss: 0.9099\n",
      "Epoch 366/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7572 - val_loss: 0.9068\n",
      "Epoch 367/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7567 - val_loss: 0.9047\n",
      "Epoch 368/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7561 - val_loss: 0.9018\n",
      "Epoch 369/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7560 - val_loss: 0.9008\n",
      "Epoch 370/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7562 - val_loss: 0.8980\n",
      "Epoch 371/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7564 - val_loss: 0.9017\n",
      "Epoch 372/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7561 - val_loss: 0.9016\n",
      "Epoch 373/500\n",
      "105/105 [==============================] - 0s 289us/sample - loss: 0.7586 - val_loss: 0.9012\n",
      "Epoch 374/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7574 - val_loss: 0.9113\n",
      "Epoch 375/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7575 - val_loss: 0.9119\n",
      "Epoch 376/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7564 - val_loss: 0.9106\n",
      "Epoch 377/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7558 - val_loss: 0.9129\n",
      "Epoch 378/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7566 - val_loss: 0.9122\n",
      "Epoch 379/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7558 - val_loss: 0.9094\n",
      "Epoch 380/500\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.7561 - val_loss: 0.9069\n",
      "Epoch 381/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7561 - val_loss: 0.9039\n",
      "Epoch 382/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7556 - val_loss: 0.9053\n",
      "Epoch 383/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 314us/sample - loss: 0.7564 - val_loss: 0.9021\n",
      "Epoch 384/500\n",
      "105/105 [==============================] - 0s 427us/sample - loss: 0.7569 - val_loss: 0.9036\n",
      "Epoch 385/500\n",
      "105/105 [==============================] - 0s 347us/sample - loss: 0.7569 - val_loss: 0.9025\n",
      "Epoch 386/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7560 - val_loss: 0.9025\n",
      "Epoch 387/500\n",
      "105/105 [==============================] - 0s 345us/sample - loss: 0.7566 - val_loss: 0.9039\n",
      "Epoch 388/500\n",
      "105/105 [==============================] - 0s 329us/sample - loss: 0.7568 - val_loss: 0.9090\n",
      "Epoch 389/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7567 - val_loss: 0.9051\n",
      "Epoch 390/500\n",
      "105/105 [==============================] - 0s 440us/sample - loss: 0.7561 - val_loss: 0.9060\n",
      "Epoch 391/500\n",
      "105/105 [==============================] - 0s 387us/sample - loss: 0.7561 - val_loss: 0.9032\n",
      "Epoch 392/500\n",
      "105/105 [==============================] - 0s 367us/sample - loss: 0.7568 - val_loss: 0.9049\n",
      "Epoch 393/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7561 - val_loss: 0.9047\n",
      "Epoch 394/500\n",
      "105/105 [==============================] - 0s 325us/sample - loss: 0.7575 - val_loss: 0.9026\n",
      "Epoch 395/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7567 - val_loss: 0.9018\n",
      "Epoch 396/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7570 - val_loss: 0.9026\n",
      "Epoch 397/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7559 - val_loss: 0.9047\n",
      "Epoch 398/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7566 - val_loss: 0.9037\n",
      "Epoch 399/500\n",
      "105/105 [==============================] - 0s 349us/sample - loss: 0.7566 - val_loss: 0.9075\n",
      "Epoch 400/500\n",
      "105/105 [==============================] - 0s 336us/sample - loss: 0.7561 - val_loss: 0.9094\n",
      "Epoch 401/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7558 - val_loss: 0.9109\n",
      "Epoch 402/500\n",
      "105/105 [==============================] - 0s 271us/sample - loss: 0.7565 - val_loss: 0.9164\n",
      "Epoch 403/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7571 - val_loss: 0.9165\n",
      "Epoch 404/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7567 - val_loss: 0.9158\n",
      "Epoch 405/500\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 0.7586 - val_loss: 0.9137\n",
      "Epoch 406/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7559 - val_loss: 0.9134\n",
      "Epoch 407/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7566 - val_loss: 0.9162\n",
      "Epoch 408/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7560 - val_loss: 0.9162\n",
      "Epoch 409/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7565 - val_loss: 0.9102\n",
      "Epoch 410/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7587 - val_loss: 0.9090\n",
      "Epoch 411/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7565 - val_loss: 0.9060\n",
      "Epoch 412/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7569 - val_loss: 0.9098\n",
      "Epoch 413/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7564 - val_loss: 0.9080\n",
      "Epoch 414/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7573 - val_loss: 0.9100\n",
      "Epoch 415/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7562 - val_loss: 0.9122\n",
      "Epoch 416/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7564 - val_loss: 0.9116\n",
      "Epoch 417/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7565 - val_loss: 0.9123\n",
      "Epoch 418/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7565 - val_loss: 0.9114\n",
      "Epoch 419/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7567 - val_loss: 0.9117\n",
      "Epoch 420/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7573 - val_loss: 0.9129\n",
      "Epoch 421/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7569 - val_loss: 0.9096\n",
      "Epoch 422/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7567 - val_loss: 0.9088\n",
      "Epoch 423/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7560 - val_loss: 0.9091\n",
      "Epoch 424/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7559 - val_loss: 0.9082\n",
      "Epoch 425/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7575 - val_loss: 0.9105\n",
      "Epoch 426/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7574 - val_loss: 0.9120\n",
      "Epoch 427/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7561 - val_loss: 0.9120\n",
      "Epoch 428/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7573 - val_loss: 0.9222\n",
      "Epoch 429/500\n",
      "105/105 [==============================] - 0s 302us/sample - loss: 0.7567 - val_loss: 0.9223\n",
      "Epoch 430/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7575 - val_loss: 0.9237\n",
      "Epoch 431/500\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 0.7573 - val_loss: 0.9256\n",
      "Epoch 432/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 0.7580 - val_loss: 0.9266\n",
      "Epoch 433/500\n",
      "105/105 [==============================] - 0s 185us/sample - loss: 0.7599 - val_loss: 0.9240\n",
      "Epoch 434/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7572 - val_loss: 0.9224\n",
      "Epoch 435/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7577 - val_loss: 0.9145\n",
      "Epoch 436/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7587 - val_loss: 0.9115\n",
      "Epoch 437/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7565 - val_loss: 0.9107\n",
      "Epoch 438/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.9143\n",
      "Epoch 439/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7563 - val_loss: 0.9114\n",
      "Epoch 440/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7570 - val_loss: 0.9090\n",
      "Epoch 441/500\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.7570 - val_loss: 0.9155\n",
      "Epoch 442/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7570 - val_loss: 0.9121\n",
      "Epoch 443/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7563 - val_loss: 0.9107\n",
      "Epoch 444/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7569 - val_loss: 0.9113\n",
      "Epoch 445/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7578 - val_loss: 0.9129\n",
      "Epoch 446/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.7572 - val_loss: 0.9099\n",
      "Epoch 447/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7571 - val_loss: 0.9124\n",
      "Epoch 448/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7571 - val_loss: 0.9121\n",
      "Epoch 449/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7576 - val_loss: 0.9102\n",
      "Epoch 450/500\n",
      "105/105 [==============================] - 0s 173us/sample - loss: 0.7567 - val_loss: 0.9104\n",
      "Epoch 451/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7563 - val_loss: 0.9088\n",
      "Epoch 452/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7581 - val_loss: 0.9085\n",
      "Epoch 453/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7572 - val_loss: 0.9078\n",
      "Epoch 454/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7578 - val_loss: 0.9057\n",
      "Epoch 455/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7559 - val_loss: 0.9035\n",
      "Epoch 456/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7568 - val_loss: 0.9050\n",
      "Epoch 457/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7567 - val_loss: 0.9096\n",
      "Epoch 458/500\n",
      "105/105 [==============================] - 0s 268us/sample - loss: 0.7571 - val_loss: 0.9075\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7561 - val_loss: 0.9083\n",
      "Epoch 460/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7570 - val_loss: 0.9126\n",
      "Epoch 461/500\n",
      "105/105 [==============================] - 0s 271us/sample - loss: 0.7575 - val_loss: 0.9172\n",
      "Epoch 462/500\n",
      "105/105 [==============================] - 0s 272us/sample - loss: 0.7572 - val_loss: 0.9149\n",
      "Epoch 463/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7568 - val_loss: 0.9111\n",
      "Epoch 464/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7562 - val_loss: 0.9114\n",
      "Epoch 465/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7580 - val_loss: 0.9089\n",
      "Epoch 466/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7569 - val_loss: 0.9084\n",
      "Epoch 467/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7574 - val_loss: 0.9089\n",
      "Epoch 468/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7573 - val_loss: 0.9087\n",
      "Epoch 469/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7572 - val_loss: 0.9063\n",
      "Epoch 470/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7567 - val_loss: 0.9086\n",
      "Epoch 471/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7572 - val_loss: 0.9073\n",
      "Epoch 472/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7575 - val_loss: 0.9069\n",
      "Epoch 473/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7571 - val_loss: 0.9055\n",
      "Epoch 474/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7570 - val_loss: 0.9071\n",
      "Epoch 475/500\n",
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7579 - val_loss: 0.9028\n",
      "Epoch 476/500\n",
      "105/105 [==============================] - 0s 314us/sample - loss: 0.7558 - val_loss: 0.9028\n",
      "Epoch 477/500\n",
      "105/105 [==============================] - 0s 259us/sample - loss: 0.7558 - val_loss: 0.9023\n",
      "Epoch 478/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7562 - val_loss: 0.9008\n",
      "Epoch 479/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7569 - val_loss: 0.9008\n",
      "Epoch 480/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7582 - val_loss: 0.9015\n",
      "Epoch 481/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7565 - val_loss: 0.9043\n",
      "Epoch 482/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7565 - val_loss: 0.9019\n",
      "Epoch 483/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7568 - val_loss: 0.9020\n",
      "Epoch 484/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7568 - val_loss: 0.9012\n",
      "Epoch 485/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7561 - val_loss: 0.9003\n",
      "Epoch 486/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7568 - val_loss: 0.9012\n",
      "Epoch 487/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7570 - val_loss: 0.8984\n",
      "Epoch 488/500\n",
      "105/105 [==============================] - 0s 328us/sample - loss: 0.7571 - val_loss: 0.9005\n",
      "Epoch 489/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7570 - val_loss: 0.9010\n",
      "Epoch 490/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7568 - val_loss: 0.8990\n",
      "Epoch 491/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7578 - val_loss: 0.9078\n",
      "Epoch 492/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7564 - val_loss: 0.9070\n",
      "Epoch 493/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7560 - val_loss: 0.9055\n",
      "Epoch 494/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7571 - val_loss: 0.9048\n",
      "Epoch 495/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7582 - val_loss: 0.9079\n",
      "Epoch 496/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7578 - val_loss: 0.9108\n",
      "Epoch 497/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7558 - val_loss: 0.9097\n",
      "Epoch 498/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7564 - val_loss: 0.9026\n",
      "Epoch 499/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7559 - val_loss: 0.9019\n",
      "Epoch 500/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7556 - val_loss: 0.9021\n"
     ]
    }
   ],
   "source": [
    "# 경사하강법으로 최적화, 손실함수 기준은 mse라는 것을 명시하며 컴파일\n",
    "model.compile(optimizer = 'sgd', loss = 'mse')\n",
    "\n",
    "# 모델 학습 시작\n",
    "history = model.fit(x_train, y_train, epochs = 500, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`compile()`** : 모델을 기계가 이해할 수 있도록 컴파일\n",
    "    * `optimizer` : 훈련 과정을 설정하는 옵티마이저를 생성, 'adam'이나 'sgd'같이 문자열로 지정할 수도 있다.\n",
    "        - 종류 : `rmsprop`, `adagrad`, `adam`,`sgd` 등...\n",
    "    * `loss` : 훈련 과정에서 사용할 손실함수를 설정\n",
    "        - 종류 : `mse`, `mae` ,`crossentropy` 등...\n",
    "    * `metrixs` : 훈련을 모니터링하기 위한 지표를 선택한다.\n",
    "\n",
    "대표적으로 사용되는 손실함수와 활성화 함수의 조합은 아래와 같음\n",
    "\n",
    "문제 유형 | 손실 함수명 | 출력층의 활성화 함수명 |\n",
    "----------|------------|-------------------------|\n",
    "회귀 문제 | mean_squard_error(평균 제곱 오차, MSE)|- |\n",
    "다중 클래스 분류 | categorical_crossentrooy(범주형 교차 엔트로피) | 소프트맥스(softmax) |\n",
    "다중 클래스 분류 | sparse_categorical_crossentropy | 소프트맥스(softmax) |\n",
    "이진 분류 | binary_crossentropy(이항 교차 엔트로피)| 시그모이드(sigmoid) |\n",
    "\n",
    "\n",
    "\n",
    "* **`fit()`**  : 모델을 학습한다. 모델이 오차로부터 매개변수를 업데이트 시키는 과정을 학습, 훈련, 또는 적합(fitting)이라고 하기도 하는데, 모델이 데이터에 적합해가는 과정이기 때문이다. 그런 의미로 fit()은 모델의 훈련을 시작한다는 의미를 가지고 있다. \n",
    "    - 첫번째 인자 : 훈련 데이터에 해당한다. (X 변수)\n",
    "    - 두번째 인자 : 지도 학습에서 레이블 데이터에 해당한다. ( 훈련 데이터의 target)\n",
    "    - epochs : 에포크. 에포크 1은 전체 데이ㅌ를 한차례 훑고 지나감을 의미. 정수값 기재 필요. 총 훈련 횟수를 정의한다. \n",
    "    - batch_size : 배치 크기. 기본값은 32. 미니 배치 경사 하강법을 사용하고 싶지 않을 경우에는 batch_size = None을 기재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = history.epoch\n",
    "\n",
    "plt.plot(epochs, history.history['loss'], label = 'Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.get(key, default=None, /)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) functional API (함수형 API)\n",
    "\n",
    "Sequential 모델은 층을 순섣로 쌓은 네트워크를 만든다. 그래서 단순한 Feed-foward 신경망밖에 만들지 못한다. 더 복잡한 모델을 만들려면 함수형 API를 사용해야 한다. 함수형 API는 입력과 출력 사이에 원하는 층을 자유롭게 조합할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "input = tf.keras.Input(shape = (1,))\n",
    "output = tf.keras.layers.Dense(1)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 코드는 다음과 같이 쓸수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(1)\n",
    "output = dense(input)\n",
    "\n",
    "## or\n",
    "## dense = tf.keras.layers.Dense(1)\n",
    "## output = dense.__call__(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential 모델에는 없었던 InputLayer는 입력데이터를 위한 층이다. 이 층은 학습되는 파라미터를 가지고 있지 않다. Sequential 모델은 보여지지 않을 뿐이지 inputLayer를 자동으로 가지고 있다.  컴파일 및 training 단계는 Sequential 모델과 완전히 동일하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/500\n",
      "105/105 [==============================] - 0s 3ms/sample - loss: 7.3637 - val_loss: 4.4197\n",
      "Epoch 2/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 6.0936 - val_loss: 3.7512\n",
      "Epoch 3/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 5.0797 - val_loss: 3.2123\n",
      "Epoch 4/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 4.2236 - val_loss: 2.7288\n",
      "Epoch 5/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 3.4980 - val_loss: 2.4037\n",
      "Epoch 6/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 2.9921 - val_loss: 2.0766\n",
      "Epoch 7/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 2.5061 - val_loss: 1.8619\n",
      "Epoch 8/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 2.1751 - val_loss: 1.6791\n",
      "Epoch 9/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 1.9031 - val_loss: 1.5444\n",
      "Epoch 10/500\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 1.6982 - val_loss: 1.4194\n",
      "Epoch 11/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 1.5141 - val_loss: 1.3293\n",
      "Epoch 12/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 1.3731 - val_loss: 1.2487\n",
      "Epoch 13/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 1.2563 - val_loss: 1.1973\n",
      "Epoch 14/500\n",
      "105/105 [==============================] - 0s 320us/sample - loss: 1.1705 - val_loss: 1.1461\n",
      "Epoch 15/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 1.0904 - val_loss: 1.1004\n",
      "Epoch 16/500\n",
      "105/105 [==============================] - 0s 263us/sample - loss: 1.0240 - val_loss: 1.0604\n",
      "Epoch 17/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.9704 - val_loss: 1.0409\n",
      "Epoch 18/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.9355 - val_loss: 1.0181\n",
      "Epoch 19/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.9020 - val_loss: 0.9968\n",
      "Epoch 20/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.8751 - val_loss: 0.9758\n",
      "Epoch 21/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.8513 - val_loss: 0.9598\n",
      "Epoch 22/500\n",
      "105/105 [==============================] - 0s 273us/sample - loss: 0.8277 - val_loss: 0.9463\n",
      "Epoch 23/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.8141 - val_loss: 0.9346\n",
      "Epoch 24/500\n",
      "105/105 [==============================] - 0s 340us/sample - loss: 0.8021 - val_loss: 0.9306\n",
      "Epoch 25/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7976 - val_loss: 0.9189\n",
      "Epoch 26/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7873 - val_loss: 0.9122\n",
      "Epoch 27/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7822 - val_loss: 0.9065\n",
      "Epoch 28/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7778 - val_loss: 0.9011\n",
      "Epoch 29/500\n",
      "105/105 [==============================] - 0s 270us/sample - loss: 0.7753 - val_loss: 0.8977\n",
      "Epoch 30/500\n",
      "105/105 [==============================] - 0s 185us/sample - loss: 0.7726 - val_loss: 0.8988\n",
      "Epoch 31/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7694 - val_loss: 0.9004\n",
      "Epoch 32/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7681 - val_loss: 0.9064\n",
      "Epoch 33/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7677 - val_loss: 0.9043\n",
      "Epoch 34/500\n",
      "105/105 [==============================] - 0s 394us/sample - loss: 0.7657 - val_loss: 0.9096\n",
      "Epoch 35/500\n",
      "105/105 [==============================] - 0s 183us/sample - loss: 0.7650 - val_loss: 0.9112\n",
      "Epoch 36/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7641 - val_loss: 0.9053\n",
      "Epoch 37/500\n",
      "105/105 [==============================] - 0s 307us/sample - loss: 0.7620 - val_loss: 0.9049\n",
      "Epoch 38/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7595 - val_loss: 0.9026\n",
      "Epoch 39/500\n",
      "105/105 [==============================] - 0s 333us/sample - loss: 0.7608 - val_loss: 0.9000\n",
      "Epoch 40/500\n",
      "105/105 [==============================] - 0s 410us/sample - loss: 0.7578 - val_loss: 0.9038\n",
      "Epoch 41/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7580 - val_loss: 0.9034\n",
      "Epoch 42/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7572 - val_loss: 0.9032\n",
      "Epoch 43/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7573 - val_loss: 0.9008\n",
      "Epoch 44/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7584 - val_loss: 0.9055\n",
      "Epoch 45/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7574 - val_loss: 0.9032\n",
      "Epoch 46/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7563 - val_loss: 0.9050\n",
      "Epoch 47/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7569 - val_loss: 0.9061\n",
      "Epoch 48/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7579 - val_loss: 0.9116\n",
      "Epoch 49/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7579 - val_loss: 0.9134\n",
      "Epoch 50/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7571 - val_loss: 0.9118\n",
      "Epoch 51/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7575 - val_loss: 0.9116\n",
      "Epoch 52/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7568 - val_loss: 0.9108\n",
      "Epoch 53/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7574 - val_loss: 0.9112\n",
      "Epoch 54/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7566 - val_loss: 0.9098\n",
      "Epoch 55/500\n",
      "105/105 [==============================] - 0s 395us/sample - loss: 0.7557 - val_loss: 0.9071\n",
      "Epoch 56/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7565 - val_loss: 0.9098\n",
      "Epoch 57/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7559 - val_loss: 0.9070\n",
      "Epoch 58/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7561 - val_loss: 0.9035\n",
      "Epoch 59/500\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.7570 - val_loss: 0.9038\n",
      "Epoch 60/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7567 - val_loss: 0.9065\n",
      "Epoch 61/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7563 - val_loss: 0.9023\n",
      "Epoch 62/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7575 - val_loss: 0.9056\n",
      "Epoch 63/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7574 - val_loss: 0.9071\n",
      "Epoch 64/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7565 - val_loss: 0.9086\n",
      "Epoch 65/500\n",
      "105/105 [==============================] - 0s 282us/sample - loss: 0.7569 - val_loss: 0.9057\n",
      "Epoch 66/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7566 - val_loss: 0.9091\n",
      "Epoch 67/500\n",
      "105/105 [==============================] - 0s 331us/sample - loss: 0.7566 - val_loss: 0.9151\n",
      "Epoch 68/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7563 - val_loss: 0.9116\n",
      "Epoch 69/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7574 - val_loss: 0.9120\n",
      "Epoch 70/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7568 - val_loss: 0.9092\n",
      "Epoch 71/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7570 - val_loss: 0.9080\n",
      "Epoch 72/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7566 - val_loss: 0.9070\n",
      "Epoch 73/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7564 - val_loss: 0.9052\n",
      "Epoch 74/500\n",
      "105/105 [==============================] - 0s 286us/sample - loss: 0.7561 - val_loss: 0.9086\n",
      "Epoch 75/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7557 - val_loss: 0.9113\n",
      "Epoch 76/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7560 - val_loss: 0.9140\n",
      "Epoch 77/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7577 - val_loss: 0.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7567 - val_loss: 0.9102\n",
      "Epoch 79/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7563 - val_loss: 0.9133\n",
      "Epoch 80/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7584 - val_loss: 0.9112\n",
      "Epoch 81/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7561 - val_loss: 0.9096\n",
      "Epoch 82/500\n",
      "105/105 [==============================] - 0s 287us/sample - loss: 0.7561 - val_loss: 0.9092\n",
      "Epoch 83/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7571 - val_loss: 0.9072\n",
      "Epoch 84/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7573 - val_loss: 0.9058\n",
      "Epoch 85/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7576 - val_loss: 0.9033\n",
      "Epoch 86/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7572 - val_loss: 0.9043\n",
      "Epoch 87/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7575 - val_loss: 0.9024\n",
      "Epoch 88/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7574 - val_loss: 0.9054\n",
      "Epoch 89/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7575 - val_loss: 0.9070\n",
      "Epoch 90/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.7576 - val_loss: 0.9060\n",
      "Epoch 91/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7565 - val_loss: 0.9074\n",
      "Epoch 92/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7559 - val_loss: 0.9086\n",
      "Epoch 93/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7560 - val_loss: 0.9099\n",
      "Epoch 94/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7572 - val_loss: 0.9096\n",
      "Epoch 95/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7565 - val_loss: 0.9079\n",
      "Epoch 96/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7563 - val_loss: 0.9067\n",
      "Epoch 97/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7568 - val_loss: 0.9056\n",
      "Epoch 98/500\n",
      "105/105 [==============================] - 0s 348us/sample - loss: 0.7570 - val_loss: 0.9004\n",
      "Epoch 99/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7577 - val_loss: 0.9051\n",
      "Epoch 100/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7576 - val_loss: 0.9013\n",
      "Epoch 101/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7562 - val_loss: 0.9029\n",
      "Epoch 102/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7564 - val_loss: 0.9003\n",
      "Epoch 103/500\n",
      "105/105 [==============================] - 0s 375us/sample - loss: 0.7561 - val_loss: 0.9037\n",
      "Epoch 104/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7558 - val_loss: 0.9031\n",
      "Epoch 105/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7558 - val_loss: 0.9026\n",
      "Epoch 106/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7559 - val_loss: 0.9044\n",
      "Epoch 107/500\n",
      "105/105 [==============================] - 0s 170us/sample - loss: 0.7575 - val_loss: 0.9032\n",
      "Epoch 108/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7568 - val_loss: 0.9072\n",
      "Epoch 109/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7558 - val_loss: 0.9076\n",
      "Epoch 110/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7565 - val_loss: 0.9030\n",
      "Epoch 111/500\n",
      "105/105 [==============================] - 0s 297us/sample - loss: 0.7575 - val_loss: 0.9031\n",
      "Epoch 112/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7557 - val_loss: 0.8997\n",
      "Epoch 113/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7578 - val_loss: 0.8977\n",
      "Epoch 114/500\n",
      "105/105 [==============================] - 0s 273us/sample - loss: 0.7561 - val_loss: 0.8967\n",
      "Epoch 115/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 0.7567 - val_loss: 0.8979\n",
      "Epoch 116/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7563 - val_loss: 0.8958\n",
      "Epoch 117/500\n",
      "105/105 [==============================] - 0s 300us/sample - loss: 0.7576 - val_loss: 0.8964\n",
      "Epoch 118/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7575 - val_loss: 0.8942\n",
      "Epoch 119/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7567 - val_loss: 0.8922\n",
      "Epoch 120/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7573 - val_loss: 0.8917\n",
      "Epoch 121/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7572 - val_loss: 0.8985\n",
      "Epoch 122/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.8991\n",
      "Epoch 123/500\n",
      "105/105 [==============================] - 0s 186us/sample - loss: 0.7575 - val_loss: 0.8992\n",
      "Epoch 124/500\n",
      "105/105 [==============================] - 0s 185us/sample - loss: 0.7578 - val_loss: 0.8998\n",
      "Epoch 125/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7567 - val_loss: 0.8973\n",
      "Epoch 126/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7580 - val_loss: 0.8979\n",
      "Epoch 127/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7580 - val_loss: 0.8994\n",
      "Epoch 128/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7571 - val_loss: 0.8989\n",
      "Epoch 129/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7564 - val_loss: 0.8956\n",
      "Epoch 130/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7567 - val_loss: 0.8964\n",
      "Epoch 131/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7569 - val_loss: 0.8950\n",
      "Epoch 132/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7567 - val_loss: 0.8969\n",
      "Epoch 133/500\n",
      "105/105 [==============================] - 0s 280us/sample - loss: 0.7557 - val_loss: 0.8979\n",
      "Epoch 134/500\n",
      "105/105 [==============================] - 0s 340us/sample - loss: 0.7562 - val_loss: 0.9004\n",
      "Epoch 135/500\n",
      "105/105 [==============================] - 0s 296us/sample - loss: 0.7563 - val_loss: 0.8978\n",
      "Epoch 136/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7571 - val_loss: 0.8970\n",
      "Epoch 137/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7571 - val_loss: 0.8959\n",
      "Epoch 138/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7563 - val_loss: 0.8971\n",
      "Epoch 139/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7565 - val_loss: 0.8979\n",
      "Epoch 140/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7560 - val_loss: 0.8980\n",
      "Epoch 141/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7561 - val_loss: 0.8994\n",
      "Epoch 142/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7559 - val_loss: 0.8966\n",
      "Epoch 143/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7565 - val_loss: 0.8951\n",
      "Epoch 144/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7586 - val_loss: 0.9010\n",
      "Epoch 145/500\n",
      "105/105 [==============================] - 0s 272us/sample - loss: 0.7568 - val_loss: 0.9029\n",
      "Epoch 146/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7566 - val_loss: 0.8987\n",
      "Epoch 147/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7561 - val_loss: 0.8952\n",
      "Epoch 148/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7572 - val_loss: 0.8946\n",
      "Epoch 149/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7577 - val_loss: 0.8950\n",
      "Epoch 150/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7568 - val_loss: 0.8923\n",
      "Epoch 151/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7574 - val_loss: 0.8926\n",
      "Epoch 152/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7583 - val_loss: 0.8933\n",
      "Epoch 153/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7568 - val_loss: 0.8964\n",
      "Epoch 154/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7567 - val_loss: 0.9010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "105/105 [==============================] - 0s 176us/sample - loss: 0.7557 - val_loss: 0.9028\n",
      "Epoch 156/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7565 - val_loss: 0.9018\n",
      "Epoch 157/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7558 - val_loss: 0.8993\n",
      "Epoch 158/500\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.7564 - val_loss: 0.8980\n",
      "Epoch 159/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7574 - val_loss: 0.8982\n",
      "Epoch 160/500\n",
      "105/105 [==============================] - 0s 178us/sample - loss: 0.7566 - val_loss: 0.8952\n",
      "Epoch 161/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7565 - val_loss: 0.8985\n",
      "Epoch 162/500\n",
      "105/105 [==============================] - 0s 174us/sample - loss: 0.7572 - val_loss: 0.8985\n",
      "Epoch 163/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7557 - val_loss: 0.8984\n",
      "Epoch 164/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7562 - val_loss: 0.9013\n",
      "Epoch 165/500\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.7568 - val_loss: 0.9023\n",
      "Epoch 166/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7567 - val_loss: 0.9030\n",
      "Epoch 167/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7572 - val_loss: 0.9042\n",
      "Epoch 168/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7555 - val_loss: 0.9030\n",
      "Epoch 169/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7558 - val_loss: 0.9013\n",
      "Epoch 170/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7572 - val_loss: 0.9003\n",
      "Epoch 171/500\n",
      "105/105 [==============================] - 0s 321us/sample - loss: 0.7581 - val_loss: 0.8967\n",
      "Epoch 172/500\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.7566 - val_loss: 0.8966\n",
      "Epoch 173/500\n",
      "105/105 [==============================] - 0s 340us/sample - loss: 0.7580 - val_loss: 0.8954\n",
      "Epoch 174/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7570 - val_loss: 0.8943\n",
      "Epoch 175/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 0.7566 - val_loss: 0.8963\n",
      "Epoch 176/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7566 - val_loss: 0.8961\n",
      "Epoch 177/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7565 - val_loss: 0.8951\n",
      "Epoch 178/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7570 - val_loss: 0.8953\n",
      "Epoch 179/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7573 - val_loss: 0.8920\n",
      "Epoch 180/500\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 0.7567 - val_loss: 0.8943\n",
      "Epoch 181/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7578 - val_loss: 0.8968\n",
      "Epoch 182/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7565 - val_loss: 0.8937\n",
      "Epoch 183/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7567 - val_loss: 0.8974\n",
      "Epoch 184/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7566 - val_loss: 0.8987\n",
      "Epoch 185/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7567 - val_loss: 0.8992\n",
      "Epoch 186/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7559 - val_loss: 0.8995\n",
      "Epoch 187/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7572 - val_loss: 0.8993\n",
      "Epoch 188/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7570 - val_loss: 0.8979\n",
      "Epoch 189/500\n",
      "105/105 [==============================] - 0s 173us/sample - loss: 0.7565 - val_loss: 0.8971\n",
      "Epoch 190/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7562 - val_loss: 0.8961\n",
      "Epoch 191/500\n",
      "105/105 [==============================] - 0s 271us/sample - loss: 0.7569 - val_loss: 0.8988\n",
      "Epoch 192/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7561 - val_loss: 0.8991\n",
      "Epoch 193/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7568 - val_loss: 0.9051\n",
      "Epoch 194/500\n",
      "105/105 [==============================] - 0s 288us/sample - loss: 0.7574 - val_loss: 0.9048\n",
      "Epoch 195/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7581 - val_loss: 0.9032\n",
      "Epoch 196/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7575 - val_loss: 0.9042\n",
      "Epoch 197/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7567 - val_loss: 0.9076\n",
      "Epoch 198/500\n",
      "105/105 [==============================] - 0s 346us/sample - loss: 0.7568 - val_loss: 0.9094\n",
      "Epoch 199/500\n",
      "105/105 [==============================] - 0s 343us/sample - loss: 0.7572 - val_loss: 0.9052\n",
      "Epoch 200/500\n",
      "105/105 [==============================] - 0s 334us/sample - loss: 0.7561 - val_loss: 0.9047\n",
      "Epoch 201/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7560 - val_loss: 0.9051\n",
      "Epoch 202/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7567 - val_loss: 0.9070\n",
      "Epoch 203/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7573 - val_loss: 0.9063\n",
      "Epoch 204/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7556 - val_loss: 0.9067\n",
      "Epoch 205/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7567 - val_loss: 0.9071\n",
      "Epoch 206/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7592 - val_loss: 0.9085\n",
      "Epoch 207/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7559 - val_loss: 0.9079\n",
      "Epoch 208/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7557 - val_loss: 0.9074\n",
      "Epoch 209/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7570 - val_loss: 0.9088\n",
      "Epoch 210/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7571 - val_loss: 0.9030\n",
      "Epoch 211/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7578 - val_loss: 0.8989\n",
      "Epoch 212/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7576 - val_loss: 0.9014\n",
      "Epoch 213/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7563 - val_loss: 0.9008\n",
      "Epoch 214/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7562 - val_loss: 0.9015\n",
      "Epoch 215/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9010\n",
      "Epoch 216/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7560 - val_loss: 0.9016\n",
      "Epoch 217/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7590 - val_loss: 0.9016\n",
      "Epoch 218/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7560 - val_loss: 0.9042\n",
      "Epoch 219/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7564 - val_loss: 0.9024\n",
      "Epoch 220/500\n",
      "105/105 [==============================] - 0s 300us/sample - loss: 0.7565 - val_loss: 0.9063\n",
      "Epoch 221/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7563 - val_loss: 0.9064\n",
      "Epoch 222/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7576 - val_loss: 0.9039\n",
      "Epoch 223/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7577 - val_loss: 0.9009\n",
      "Epoch 224/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7576 - val_loss: 0.9033\n",
      "Epoch 225/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7575 - val_loss: 0.9040\n",
      "Epoch 226/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7603 - val_loss: 0.9039\n",
      "Epoch 227/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7579 - val_loss: 0.9051\n",
      "Epoch 228/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7569 - val_loss: 0.9051\n",
      "Epoch 229/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7562 - val_loss: 0.9048\n",
      "Epoch 230/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7566 - val_loss: 0.9050\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7567 - val_loss: 0.9045\n",
      "Epoch 232/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7564 - val_loss: 0.9041\n",
      "Epoch 233/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7566 - val_loss: 0.9013\n",
      "Epoch 234/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7561 - val_loss: 0.8993\n",
      "Epoch 235/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7569 - val_loss: 0.8985\n",
      "Epoch 236/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7567 - val_loss: 0.8950\n",
      "Epoch 237/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7586 - val_loss: 0.8977\n",
      "Epoch 238/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7559 - val_loss: 0.8985\n",
      "Epoch 239/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7578 - val_loss: 0.8972\n",
      "Epoch 240/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7564 - val_loss: 0.8972\n",
      "Epoch 241/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7569 - val_loss: 0.8964\n",
      "Epoch 242/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7570 - val_loss: 0.8956\n",
      "Epoch 243/500\n",
      "105/105 [==============================] - 0s 288us/sample - loss: 0.7568 - val_loss: 0.8997\n",
      "Epoch 244/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7568 - val_loss: 0.9055\n",
      "Epoch 245/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.915 - 0s 228us/sample - loss: 0.7560 - val_loss: 0.9060\n",
      "Epoch 246/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7563 - val_loss: 0.9013\n",
      "Epoch 247/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7559 - val_loss: 0.9003\n",
      "Epoch 248/500\n",
      "105/105 [==============================] - 0s 178us/sample - loss: 0.7555 - val_loss: 0.9020\n",
      "Epoch 249/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7563 - val_loss: 0.9019\n",
      "Epoch 250/500\n",
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7567 - val_loss: 0.9040\n",
      "Epoch 251/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7559 - val_loss: 0.9067\n",
      "Epoch 252/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7561 - val_loss: 0.9116\n",
      "Epoch 253/500\n",
      "105/105 [==============================] - 0s 183us/sample - loss: 0.7562 - val_loss: 0.9102\n",
      "Epoch 254/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7574 - val_loss: 0.9060\n",
      "Epoch 255/500\n",
      "105/105 [==============================] - 0s 359us/sample - loss: 0.7566 - val_loss: 0.9117\n",
      "Epoch 256/500\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.7573 - val_loss: 0.9122\n",
      "Epoch 257/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7566 - val_loss: 0.9113\n",
      "Epoch 258/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7575 - val_loss: 0.9105\n",
      "Epoch 259/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7567 - val_loss: 0.9088\n",
      "Epoch 260/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7564 - val_loss: 0.9095\n",
      "Epoch 261/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7565 - val_loss: 0.9096\n",
      "Epoch 262/500\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.7569 - val_loss: 0.9114\n",
      "Epoch 263/500\n",
      "105/105 [==============================] - 0s 420us/sample - loss: 0.7573 - val_loss: 0.9099\n",
      "Epoch 264/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7569 - val_loss: 0.9112\n",
      "Epoch 265/500\n",
      "105/105 [==============================] - 0s 259us/sample - loss: 0.7569 - val_loss: 0.9073\n",
      "Epoch 266/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7571 - val_loss: 0.9033\n",
      "Epoch 267/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7558 - val_loss: 0.9017\n",
      "Epoch 268/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7568 - val_loss: 0.9022\n",
      "Epoch 269/500\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.7561 - val_loss: 0.9046\n",
      "Epoch 270/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7566 - val_loss: 0.9071\n",
      "Epoch 271/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7561 - val_loss: 0.9039\n",
      "Epoch 272/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.7576 - val_loss: 0.9098\n",
      "Epoch 273/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7572 - val_loss: 0.9084\n",
      "Epoch 274/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7567 - val_loss: 0.9075\n",
      "Epoch 275/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7579 - val_loss: 0.9082\n",
      "Epoch 276/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7566 - val_loss: 0.9140\n",
      "Epoch 277/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7583 - val_loss: 0.9128\n",
      "Epoch 278/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7560 - val_loss: 0.9140\n",
      "Epoch 279/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7565 - val_loss: 0.9134\n",
      "Epoch 280/500\n",
      "105/105 [==============================] - 0s 169us/sample - loss: 0.7570 - val_loss: 0.9148\n",
      "Epoch 281/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7571 - val_loss: 0.9118\n",
      "Epoch 282/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7567 - val_loss: 0.9101\n",
      "Epoch 283/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7567 - val_loss: 0.9154\n",
      "Epoch 284/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7567 - val_loss: 0.9211\n",
      "Epoch 285/500\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.7577 - val_loss: 0.9171\n",
      "Epoch 286/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7572 - val_loss: 0.9167\n",
      "Epoch 287/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7576 - val_loss: 0.9099\n",
      "Epoch 288/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7570 - val_loss: 0.9072\n",
      "Epoch 289/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.7562 - val_loss: 0.9050\n",
      "Epoch 290/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7574 - val_loss: 0.9101\n",
      "Epoch 291/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7567 - val_loss: 0.9182\n",
      "Epoch 292/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7570 - val_loss: 0.9195\n",
      "Epoch 293/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7576 - val_loss: 0.9231\n",
      "Epoch 294/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7586 - val_loss: 0.9204\n",
      "Epoch 295/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7573 - val_loss: 0.9193\n",
      "Epoch 296/500\n",
      "105/105 [==============================] - 0s 172us/sample - loss: 0.7569 - val_loss: 0.9131\n",
      "Epoch 297/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7564 - val_loss: 0.9175\n",
      "Epoch 298/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.9149\n",
      "Epoch 299/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7572 - val_loss: 0.9183\n",
      "Epoch 300/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7569 - val_loss: 0.9164\n",
      "Epoch 301/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7566 - val_loss: 0.9138\n",
      "Epoch 302/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7563 - val_loss: 0.9140\n",
      "Epoch 303/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7575 - val_loss: 0.9170\n",
      "Epoch 304/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7572 - val_loss: 0.9173\n",
      "Epoch 305/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7571 - val_loss: 0.9123\n",
      "Epoch 306/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7563 - val_loss: 0.9127\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 321us/sample - loss: 0.7565 - val_loss: 0.9112\n",
      "Epoch 308/500\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.7570 - val_loss: 0.9126\n",
      "Epoch 309/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7563 - val_loss: 0.9123\n",
      "Epoch 310/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7576 - val_loss: 0.9242\n",
      "Epoch 311/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7577 - val_loss: 0.9235\n",
      "Epoch 312/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7578 - val_loss: 0.9185\n",
      "Epoch 313/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7570 - val_loss: 0.9147\n",
      "Epoch 314/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7567 - val_loss: 0.9150\n",
      "Epoch 315/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7567 - val_loss: 0.9168\n",
      "Epoch 316/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7568 - val_loss: 0.9145\n",
      "Epoch 317/500\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.7565 - val_loss: 0.9139\n",
      "Epoch 318/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7569 - val_loss: 0.9127\n",
      "Epoch 319/500\n",
      "105/105 [==============================] - 0s 307us/sample - loss: 0.7585 - val_loss: 0.9165\n",
      "Epoch 320/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7572 - val_loss: 0.9165\n",
      "Epoch 321/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7588 - val_loss: 0.9196\n",
      "Epoch 322/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7566 - val_loss: 0.9169\n",
      "Epoch 323/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7568 - val_loss: 0.9145\n",
      "Epoch 324/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7569 - val_loss: 0.9114\n",
      "Epoch 325/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7573 - val_loss: 0.9127\n",
      "Epoch 326/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7564 - val_loss: 0.9106\n",
      "Epoch 327/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7562 - val_loss: 0.9120\n",
      "Epoch 328/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7565 - val_loss: 0.9113\n",
      "Epoch 329/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7566 - val_loss: 0.9084\n",
      "Epoch 330/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7568 - val_loss: 0.9033\n",
      "Epoch 331/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7566 - val_loss: 0.9031\n",
      "Epoch 332/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9052\n",
      "Epoch 333/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7562 - val_loss: 0.9083\n",
      "Epoch 334/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7569 - val_loss: 0.9036\n",
      "Epoch 335/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7572 - val_loss: 0.9052\n",
      "Epoch 336/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7583 - val_loss: 0.9036\n",
      "Epoch 337/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7579 - val_loss: 0.9003\n",
      "Epoch 338/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7566 - val_loss: 0.9019\n",
      "Epoch 339/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7570 - val_loss: 0.9032\n",
      "Epoch 340/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7563 - val_loss: 0.9021\n",
      "Epoch 341/500\n",
      "105/105 [==============================] - 0s 319us/sample - loss: 0.7560 - val_loss: 0.9057\n",
      "Epoch 342/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7558 - val_loss: 0.9050\n",
      "Epoch 343/500\n",
      "105/105 [==============================] - 0s 330us/sample - loss: 0.7574 - val_loss: 0.9020\n",
      "Epoch 344/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7562 - val_loss: 0.9057\n",
      "Epoch 345/500\n",
      "105/105 [==============================] - 0s 324us/sample - loss: 0.7562 - val_loss: 0.9123\n",
      "Epoch 346/500\n",
      "105/105 [==============================] - 0s 197us/sample - loss: 0.7568 - val_loss: 0.9120\n",
      "Epoch 347/500\n",
      "105/105 [==============================] - 0s 173us/sample - loss: 0.7565 - val_loss: 0.9083\n",
      "Epoch 348/500\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7565 - val_loss: 0.9102\n",
      "Epoch 349/500\n",
      "105/105 [==============================] - 0s 415us/sample - loss: 0.7567 - val_loss: 0.9094\n",
      "Epoch 350/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7566 - val_loss: 0.9098\n",
      "Epoch 351/500\n",
      "105/105 [==============================] - 0s 319us/sample - loss: 0.7561 - val_loss: 0.9050\n",
      "Epoch 352/500\n",
      "105/105 [==============================] - 0s 383us/sample - loss: 0.7569 - val_loss: 0.9048\n",
      "Epoch 353/500\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.7558 - val_loss: 0.9078\n",
      "Epoch 354/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7562 - val_loss: 0.9053\n",
      "Epoch 355/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7562 - val_loss: 0.9027\n",
      "Epoch 356/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7565 - val_loss: 0.9023\n",
      "Epoch 357/500\n",
      "105/105 [==============================] - 0s 186us/sample - loss: 0.7564 - val_loss: 0.9058\n",
      "Epoch 358/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7576 - val_loss: 0.9059\n",
      "Epoch 359/500\n",
      "105/105 [==============================] - 0s 252us/sample - loss: 0.7563 - val_loss: 0.9075\n",
      "Epoch 360/500\n",
      "105/105 [==============================] - 0s 263us/sample - loss: 0.7575 - val_loss: 0.9064\n",
      "Epoch 361/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7575 - val_loss: 0.9021\n",
      "Epoch 362/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7568 - val_loss: 0.9037\n",
      "Epoch 363/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7562 - val_loss: 0.9021\n",
      "Epoch 364/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7577 - val_loss: 0.8961\n",
      "Epoch 365/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7580 - val_loss: 0.8951\n",
      "Epoch 366/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7575 - val_loss: 0.8937\n",
      "Epoch 367/500\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7568 - val_loss: 0.8963\n",
      "Epoch 368/500\n",
      "105/105 [==============================] - 0s 174us/sample - loss: 0.7562 - val_loss: 0.8974\n",
      "Epoch 369/500\n",
      "105/105 [==============================] - 0s 293us/sample - loss: 0.7564 - val_loss: 0.8943\n",
      "Epoch 370/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7572 - val_loss: 0.8915\n",
      "Epoch 371/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7572 - val_loss: 0.8952\n",
      "Epoch 372/500\n",
      "105/105 [==============================] - 0s 354us/sample - loss: 0.7589 - val_loss: 0.8977\n",
      "Epoch 373/500\n",
      "105/105 [==============================] - 0s 276us/sample - loss: 0.7570 - val_loss: 0.9037\n",
      "Epoch 374/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7565 - val_loss: 0.8992\n",
      "Epoch 375/500\n",
      "105/105 [==============================] - 0s 259us/sample - loss: 0.7567 - val_loss: 0.8981\n",
      "Epoch 376/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7568 - val_loss: 0.9033\n",
      "Epoch 377/500\n",
      "105/105 [==============================] - 0s 172us/sample - loss: 0.7557 - val_loss: 0.9049\n",
      "Epoch 378/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7566 - val_loss: 0.9034\n",
      "Epoch 379/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7562 - val_loss: 0.9018\n",
      "Epoch 380/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7582 - val_loss: 0.9035\n",
      "Epoch 381/500\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.7562 - val_loss: 0.9030\n",
      "Epoch 382/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7562 - val_loss: 0.9025\n",
      "Epoch 383/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 314us/sample - loss: 0.7565 - val_loss: 0.9073\n",
      "Epoch 384/500\n",
      "105/105 [==============================] - 0s 276us/sample - loss: 0.7569 - val_loss: 0.9057\n",
      "Epoch 385/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7585 - val_loss: 0.9090\n",
      "Epoch 386/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7563 - val_loss: 0.9128\n",
      "Epoch 387/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7562 - val_loss: 0.9119\n",
      "Epoch 388/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7567 - val_loss: 0.9092\n",
      "Epoch 389/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7566 - val_loss: 0.9089\n",
      "Epoch 390/500\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.7560 - val_loss: 0.9080\n",
      "Epoch 391/500\n",
      "105/105 [==============================] - 0s 268us/sample - loss: 0.7565 - val_loss: 0.9070\n",
      "Epoch 392/500\n",
      "105/105 [==============================] - 0s 156us/sample - loss: 0.7568 - val_loss: 0.9093\n",
      "Epoch 393/500\n",
      "105/105 [==============================] - 0s 164us/sample - loss: 0.7559 - val_loss: 0.9098\n",
      "Epoch 394/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7567 - val_loss: 0.9179\n",
      "Epoch 395/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7572 - val_loss: 0.9116\n",
      "Epoch 396/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7562 - val_loss: 0.9104\n",
      "Epoch 397/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7561 - val_loss: 0.9092\n",
      "Epoch 398/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7572 - val_loss: 0.9043\n",
      "Epoch 399/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7563 - val_loss: 0.9040\n",
      "Epoch 400/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 0.7561 - val_loss: 0.9053\n",
      "Epoch 401/500\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.7566 - val_loss: 0.9061\n",
      "Epoch 402/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7571 - val_loss: 0.9104\n",
      "Epoch 403/500\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.7582 - val_loss: 0.9073\n",
      "Epoch 404/500\n",
      "105/105 [==============================] - 0s 170us/sample - loss: 0.7579 - val_loss: 0.9050\n",
      "Epoch 405/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7573 - val_loss: 0.9026\n",
      "Epoch 406/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7583 - val_loss: 0.9029\n",
      "Epoch 407/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7570 - val_loss: 0.9024\n",
      "Epoch 408/500\n",
      "105/105 [==============================] - 0s 301us/sample - loss: 0.7567 - val_loss: 0.9011\n",
      "Epoch 409/500\n",
      "105/105 [==============================] - 0s 281us/sample - loss: 0.7559 - val_loss: 0.9022\n",
      "Epoch 410/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7576 - val_loss: 0.9004\n",
      "Epoch 411/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7562 - val_loss: 0.8997\n",
      "Epoch 412/500\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.7558 - val_loss: 0.8970\n",
      "Epoch 413/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7560 - val_loss: 0.8964\n",
      "Epoch 414/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7562 - val_loss: 0.8964\n",
      "Epoch 415/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7580 - val_loss: 0.8962\n",
      "Epoch 416/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7584 - val_loss: 0.8957\n",
      "Epoch 417/500\n",
      "105/105 [==============================] - 0s 177us/sample - loss: 0.7561 - val_loss: 0.8970\n",
      "Epoch 418/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7572 - val_loss: 0.9044\n",
      "Epoch 419/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7567 - val_loss: 0.9063\n",
      "Epoch 420/500\n",
      "105/105 [==============================] - 0s 308us/sample - loss: 0.7568 - val_loss: 0.9061\n",
      "Epoch 421/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7562 - val_loss: 0.9056\n",
      "Epoch 422/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7564 - val_loss: 0.9021\n",
      "Epoch 423/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7566 - val_loss: 0.9005\n",
      "Epoch 424/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7562 - val_loss: 0.9036\n",
      "Epoch 425/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7565 - val_loss: 0.9054\n",
      "Epoch 426/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7563 - val_loss: 0.9065\n",
      "Epoch 427/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7563 - val_loss: 0.9025\n",
      "Epoch 428/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7567 - val_loss: 0.8975\n",
      "Epoch 429/500\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.7567 - val_loss: 0.8992\n",
      "Epoch 430/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7571 - val_loss: 0.9021\n",
      "Epoch 431/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7566 - val_loss: 0.9056\n",
      "Epoch 432/500\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.7571 - val_loss: 0.9109\n",
      "Epoch 433/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7565 - val_loss: 0.9118\n",
      "Epoch 434/500\n",
      "105/105 [==============================] - 0s 287us/sample - loss: 0.7577 - val_loss: 0.9119\n",
      "Epoch 435/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7585 - val_loss: 0.9054\n",
      "Epoch 436/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7572 - val_loss: 0.9080\n",
      "Epoch 437/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7574 - val_loss: 0.9090\n",
      "Epoch 438/500\n",
      "105/105 [==============================] - 0s 282us/sample - loss: 0.7569 - val_loss: 0.9115\n",
      "Epoch 439/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7586 - val_loss: 0.9105\n",
      "Epoch 440/500\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.7566 - val_loss: 0.9085\n",
      "Epoch 441/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7559 - val_loss: 0.9074\n",
      "Epoch 442/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7566 - val_loss: 0.9044\n",
      "Epoch 443/500\n",
      "105/105 [==============================] - 0s 281us/sample - loss: 0.7567 - val_loss: 0.9043\n",
      "Epoch 444/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7577 - val_loss: 0.9059\n",
      "Epoch 445/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7559 - val_loss: 0.9089\n",
      "Epoch 446/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7573 - val_loss: 0.9026\n",
      "Epoch 447/500\n",
      "105/105 [==============================] - 0s 263us/sample - loss: 0.7593 - val_loss: 0.9019\n",
      "Epoch 448/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7561 - val_loss: 0.9034\n",
      "Epoch 449/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7562 - val_loss: 0.8991\n",
      "Epoch 450/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7567 - val_loss: 0.8965\n",
      "Epoch 451/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7564 - val_loss: 0.8946\n",
      "Epoch 452/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7565 - val_loss: 0.8994\n",
      "Epoch 453/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7562 - val_loss: 0.9033\n",
      "Epoch 454/500\n",
      "105/105 [==============================] - 0s 281us/sample - loss: 0.7560 - val_loss: 0.9048\n",
      "Epoch 455/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7584 - val_loss: 0.9028\n",
      "Epoch 456/500\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.7556 - val_loss: 0.9017\n",
      "Epoch 457/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7568 - val_loss: 0.9034\n",
      "Epoch 458/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7568 - val_loss: 0.9045\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7577 - val_loss: 0.9092\n",
      "Epoch 460/500\n",
      "105/105 [==============================] - 0s 251us/sample - loss: 0.7559 - val_loss: 0.9089\n",
      "Epoch 461/500\n",
      "105/105 [==============================] - 0s 320us/sample - loss: 0.7560 - val_loss: 0.9116\n",
      "Epoch 462/500\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.7575 - val_loss: 0.9103\n",
      "Epoch 463/500\n",
      "105/105 [==============================] - 0s 293us/sample - loss: 0.7559 - val_loss: 0.9126\n",
      "Epoch 464/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7565 - val_loss: 0.9125\n",
      "Epoch 465/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7567 - val_loss: 0.9110\n",
      "Epoch 466/500\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.7559 - val_loss: 0.9140\n",
      "Epoch 467/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7563 - val_loss: 0.9123\n",
      "Epoch 468/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7564 - val_loss: 0.9102\n",
      "Epoch 469/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7560 - val_loss: 0.9127\n",
      "Epoch 470/500\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.7574 - val_loss: 0.9167\n",
      "Epoch 471/500\n",
      "105/105 [==============================] - 0s 186us/sample - loss: 0.7587 - val_loss: 0.9170\n",
      "Epoch 472/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7597 - val_loss: 0.9142\n",
      "Epoch 473/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7563 - val_loss: 0.9127\n",
      "Epoch 474/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7570 - val_loss: 0.9136\n",
      "Epoch 475/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7565 - val_loss: 0.9114\n",
      "Epoch 476/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7563 - val_loss: 0.9095\n",
      "Epoch 477/500\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.7579 - val_loss: 0.9077\n",
      "Epoch 478/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7565 - val_loss: 0.9061\n",
      "Epoch 479/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7566 - val_loss: 0.9015\n",
      "Epoch 480/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7572 - val_loss: 0.9005\n",
      "Epoch 481/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7577 - val_loss: 0.9060\n",
      "Epoch 482/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7572 - val_loss: 0.9039\n",
      "Epoch 483/500\n",
      "105/105 [==============================] - 0s 174us/sample - loss: 0.7569 - val_loss: 0.9030\n",
      "Epoch 484/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7578 - val_loss: 0.9022\n",
      "Epoch 485/500\n",
      "105/105 [==============================] - 0s 259us/sample - loss: 0.7584 - val_loss: 0.9028\n",
      "Epoch 486/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7584 - val_loss: 0.9071\n",
      "Epoch 487/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7568 - val_loss: 0.9001\n",
      "Epoch 488/500\n",
      "105/105 [==============================] - 0s 320us/sample - loss: 0.7569 - val_loss: 0.9007\n",
      "Epoch 489/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7566 - val_loss: 0.9033\n",
      "Epoch 490/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7570 - val_loss: 0.9036\n",
      "Epoch 491/500\n",
      "105/105 [==============================] - 0s 350us/sample - loss: 0.7569 - val_loss: 0.9046\n",
      "Epoch 492/500\n",
      "105/105 [==============================] - 0s 330us/sample - loss: 0.7565 - val_loss: 0.9027\n",
      "Epoch 493/500\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.7577 - val_loss: 0.9082\n",
      "Epoch 494/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7561 - val_loss: 0.9065\n",
      "Epoch 495/500\n",
      "105/105 [==============================] - 0s 324us/sample - loss: 0.7569 - val_loss: 0.9081\n",
      "Epoch 496/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7563 - val_loss: 0.9097\n",
      "Epoch 497/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7576 - val_loss: 0.9049\n",
      "Epoch 498/500\n",
      "105/105 [==============================] - 0s 377us/sample - loss: 0.7568 - val_loss: 0.9070\n",
      "Epoch 499/500\n",
      "105/105 [==============================] - 0s 384us/sample - loss: 0.7569 - val_loss: 0.9128\n",
      "Epoch 500/500\n",
      "105/105 [==============================] - 0s 318us/sample - loss: 0.7560 - val_loss: 0.9145\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'sgd', loss = 'mse')\n",
    "history = model.fit(x_train, y_train, epochs = 500, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = history.epoch\n",
    "\n",
    "plt.plot(epochs, history.history['loss'], label = 'Training loss')\n",
    "plt.plot(epochs, history.history['val_loss'], label = 'Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) tf.keras 모델의 저장과 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model save\n",
    "model.save_weights('simple_weights.h5')\n",
    "\n",
    "## model load\n",
    "model.load_weights('simple_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "50/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 80us/sample - loss: 0.7906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9460647058486938"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  모델 실행\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
